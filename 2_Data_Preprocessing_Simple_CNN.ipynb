{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Data Preprocessing and Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will focus on preprocessing data to bring them optimal size and format using data augmentation parameters. Once the preprocessing complete, I will train various Tensorflow Keras \"sequential\" and \"\"convolutional\" networks as well as pre-trained models to sample transfer learning to come up with best results. As the classes are imbalanced, my success metrics should be Precision and Recall. Specifically Recall score is the most important as our goal is to focus on minimizing false negative rates to not classify a patient as healthy while in fact they have pneumonia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, ZeroPadding2D, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, DirectoryIterator\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './data/chest_xray/train/'\n",
    "test_dir = './data/chest_xray/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['normal', 'pneumonia']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I manually checked the images and found that there are a lot of variations for such a small dataset. The hight/width ratio, zooming range, angle of the body etc features differ among differen Xray images. Even the physical dimensions of images are vastly different. This makes it harder to train a model that will give high accuracy rate. I decided to use generator class to generate more images within train data with optimal rotation_range, shear_range, zoom_range, horizontal_flip (mirroring randomly selected images) to get additional observations to train the model with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation configuration to be used while training\n",
    "train_generator = ImageDataGenerator(\n",
    "                            rotation_range=20,\n",
    "                            width_shift_range=0.25,\n",
    "                            height_shift_range=0.25,\n",
    "                            rescale=1./255,\n",
    "                            shear_range=0.25,\n",
    "                            zoom_range=0.25,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode='nearest'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will resize images to 224x224 px value and turn them to grayscale to only save the brightness and get rid of RGB values as the images are alrady provided as grayscale. It will help the train process run faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5232 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_set = DirectoryIterator(train_dir,\n",
    "                             train_generator,\n",
    "                             target_size = (224, 224),\n",
    "                             color_mode = 'rgb',\n",
    "                             batch_size = 16,\n",
    "                             classes=classes,\n",
    "                             class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation configuration to be used for validation\n",
    "test_generator = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = DirectoryIterator(test_dir,\n",
    "                             test_generator,\n",
    "                             target_size = (224, 224),\n",
    "                             color_mode = 'rgb',\n",
    "                             batch_size = 16, # set batch size a number that divides sample size\n",
    "                             classes=classes,\n",
    "                             class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor size of train images\n",
    "train_set.image_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor size of test images\n",
    "test_set.image_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(train_set.filenames)\n",
    "test_size = len(test_set.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5232, 624)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #1 Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sequential model\n",
    "model1 = Sequential()\n",
    "\n",
    "# Add flatten layer as input layer\n",
    "model1.add(Flatten(input_shape = train_set.image_shape))\n",
    "\n",
    "# Add a densely-connected layer with X neurons\n",
    "model1.add(Dense(units = 32,\n",
    "                 activation='relu'))\n",
    "\n",
    "# Add regularization\n",
    "model1.add(Dropout(rate = 0.30))\n",
    "\n",
    "# Add a second densely-connected layer with X neurons\n",
    "model1.add(Dense(units = 256,\n",
    "                activation='relu'))\n",
    "\n",
    "# Add a third densely-connected layer with X neurons\n",
    "model1.add(Dense(units = 512,\n",
    "                 activation='relu'))\n",
    "\n",
    "# Add regularization\n",
    "model1.add(Dropout(rate = 0.10))\n",
    "\n",
    "# Add a fourth densely-connected layer with X neurons\n",
    "model1.add(Dense(units = 16,\n",
    "                 activation='relu'))\n",
    "\n",
    "# Add output layer\n",
    "model1.add(Dense(units = 2,\n",
    "    activation='sigmoid'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "opt = SGD(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the first model\n",
    "model1.compile(optimizer = opt,\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping\n",
    "early_stopping_monitor = EarlyStopping(patience = 5, \n",
    "                                       monitor = \"val_accuracy\", \n",
    "                                       mode=\"max\", \n",
    "                                       verbose = 2)\n",
    "\n",
    "# Define batch size (a divisor of test sample size)\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "327/327 - 152s - loss: 0.5882 - accuracy: 0.7343 - val_loss: 0.6722 - val_accuracy: 0.6250\n",
      "Epoch 2/25\n",
      "327/327 - 135s - loss: 0.5655 - accuracy: 0.7422 - val_loss: 0.6621 - val_accuracy: 0.6250\n",
      "Epoch 3/25\n",
      "327/327 - 114s - loss: 0.5613 - accuracy: 0.7422 - val_loss: 0.7106 - val_accuracy: 0.6250\n",
      "Epoch 4/25\n",
      "327/327 - 98s - loss: 0.5510 - accuracy: 0.7424 - val_loss: 0.6798 - val_accuracy: 0.6250\n",
      "Epoch 5/25\n",
      "327/327 - 97s - loss: 0.5452 - accuracy: 0.7427 - val_loss: 0.6528 - val_accuracy: 0.6250\n",
      "Epoch 6/25\n",
      "327/327 - 112s - loss: 0.5490 - accuracy: 0.7422 - val_loss: 0.6503 - val_accuracy: 0.6250\n",
      "Epoch 7/25\n",
      "327/327 - 97s - loss: 0.5444 - accuracy: 0.7422 - val_loss: 0.6705 - val_accuracy: 0.6250\n",
      "Epoch 8/25\n",
      "327/327 - 100s - loss: 0.5347 - accuracy: 0.7422 - val_loss: 0.6508 - val_accuracy: 0.6250\n",
      "Epoch 9/25\n",
      "327/327 - 119s - loss: 0.5366 - accuracy: 0.7420 - val_loss: 0.6538 - val_accuracy: 0.6250\n",
      "Epoch 10/25\n",
      "327/327 - 97s - loss: 0.5422 - accuracy: 0.7416 - val_loss: 0.6438 - val_accuracy: 0.6250\n",
      "Epoch 11/25\n",
      "327/327 - 97s - loss: 0.5348 - accuracy: 0.7448 - val_loss: 0.6517 - val_accuracy: 0.6298\n",
      "Epoch 12/25\n",
      "327/327 - 95s - loss: 0.5308 - accuracy: 0.7448 - val_loss: 0.6312 - val_accuracy: 0.6250\n",
      "Epoch 13/25\n",
      "327/327 - 96s - loss: 0.5368 - accuracy: 0.7439 - val_loss: 0.6410 - val_accuracy: 0.6250\n",
      "Epoch 14/25\n",
      "327/327 - 98s - loss: 0.5309 - accuracy: 0.7460 - val_loss: 0.6245 - val_accuracy: 0.6250\n",
      "Epoch 15/25\n",
      "327/327 - 106s - loss: 0.5294 - accuracy: 0.7469 - val_loss: 0.6467 - val_accuracy: 0.6234\n",
      "Epoch 16/25\n",
      "327/327 - 94s - loss: 0.5248 - accuracy: 0.7508 - val_loss: 0.6632 - val_accuracy: 0.6250\n",
      "Epoch 17/25\n",
      "327/327 - 95s - loss: 0.5271 - accuracy: 0.7473 - val_loss: 0.6489 - val_accuracy: 0.6250\n",
      "Epoch 18/25\n",
      "327/327 - 93s - loss: 0.5219 - accuracy: 0.7479 - val_loss: 0.6266 - val_accuracy: 0.6218\n",
      "Epoch 19/25\n",
      "327/327 - 96s - loss: 0.5266 - accuracy: 0.7500 - val_loss: 0.6050 - val_accuracy: 0.6282\n",
      "Epoch 20/25\n",
      "327/327 - 95s - loss: 0.5327 - accuracy: 0.7483 - val_loss: 0.6513 - val_accuracy: 0.6314\n",
      "Epoch 21/25\n",
      "327/327 - 93s - loss: 0.5271 - accuracy: 0.7469 - val_loss: 0.5976 - val_accuracy: 0.6282\n",
      "Epoch 22/25\n",
      "327/327 - 95s - loss: 0.5178 - accuracy: 0.7525 - val_loss: 0.6491 - val_accuracy: 0.6282\n",
      "Epoch 23/25\n",
      "327/327 - 94s - loss: 0.5196 - accuracy: 0.7506 - val_loss: 0.6224 - val_accuracy: 0.6266\n",
      "Epoch 24/25\n",
      "327/327 - 95s - loss: 0.5230 - accuracy: 0.7511 - val_loss: 0.6415 - val_accuracy: 0.6250\n",
      "Epoch 25/25\n",
      "327/327 - 94s - loss: 0.5170 - accuracy: 0.7500 - val_loss: 0.6273 - val_accuracy: 0.6298\n"
     ]
    }
   ],
   "source": [
    "# Fit model on training data\n",
    "history = model1.fit_generator(generator = train_set,\n",
    "                              validation_data = test_set,\n",
    "                              epochs = 25,\n",
    "#                               callbacks=[early_stopping_monitor],\n",
    "                              steps_per_epoch = train_size/batch_size, \n",
    "                              validation_steps = test_size/batch_size,\n",
    "                              shuffle=False,\n",
    "                              verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check out our train loss & accuracy and test loss & accuracy over epochs.\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "test_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Generate line plot of training, testing loss & accuracy over epochs.\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_loss, label='Training Loss', color='blue')\n",
    "plt.plot(test_loss, label='Testing Loss', color='red')\n",
    "plt.plot(train_accuracy, label='Training Accuracy', color='purple')\n",
    "plt.plot(test_accuracy, label='Testing Accuracy', color='orange')\n",
    "plt.legend(fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 - 5s\n"
     ]
    }
   ],
   "source": [
    "predictions = model1.predict_generator(generator = test_set, \n",
    "                                       verbose = 2,\n",
    "                                       steps = test_size/batch_size\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.argmax(predictions, axis = 1) \n",
    "y_true = test_set.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_true) == len(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "CM = confusion_matrix(y_true, y_hat)\n",
    "fig, ax = plot_confusion_matrix(conf_mat=CM,  figsize=(5, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precison of chest X-ray for pneumonia:0.62\n",
      "Recall of chest X-ray for pneumonia:0.99\n"
     ]
    }
   ],
   "source": [
    "true_negative, false_positive, false_negative, true_positive  = CM.ravel()\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "recall = true_positive / (true_positive + false_negative)\n",
    "\n",
    "print('Precison of chest X-ray for pneumonia:{:.2f}'.format(precision))\n",
    "print('Recall of chest X-ray for pneumonia:{:.2f}'.format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #2 Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sequential model\n",
    "model2 = Sequential()\n",
    "\n",
    "# Add convolution and pooling as input layer\n",
    "model2.add(Conv2D(filters = 32, # number of filters\n",
    "                 kernel_size = (3, 3), # height/width of filter\n",
    "                 input_shape = train_set.image_shape, # shape of input (image)\n",
    "                 activation = 'relu')) # activation function\n",
    "\n",
    "model2.add(MaxPooling2D(pool_size = (2, 2))) # dimensions of region of pooling\n",
    "\n",
    "# Add a second convolutional layer\n",
    "model2.add(Conv2D(filters = 32, \n",
    "                 kernel_size = (3, 3),\n",
    "                 activation = 'relu'))\n",
    "\n",
    "model2.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Add a flattening layer\n",
    "model2.add(Flatten())\n",
    "\n",
    "# Add a densely-connected layer\n",
    "model2.add(Dense(units = 128,\n",
    "                activation = 'relu'))\n",
    "\n",
    "# Add output layer\n",
    "model2.add(Dense(units = 2,\n",
    "                activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 222, 222, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 109, 109, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 54, 54, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 93312)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               11944064  \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 11,954,466\n",
      "Trainable params: 11,954,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer = 'adam',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "327/327 - 256s - loss: 0.4650 - accuracy: 0.7874 - val_loss: 0.6564 - val_accuracy: 0.6394\n",
      "Epoch 2/10\n",
      "327/327 - 251s - loss: 0.3634 - accuracy: 0.8243 - val_loss: 0.4823 - val_accuracy: 0.7492\n",
      "Epoch 3/10\n",
      "327/327 - 251s - loss: 0.3414 - accuracy: 0.8418 - val_loss: 0.5809 - val_accuracy: 0.7364\n",
      "Epoch 4/10\n",
      "327/327 - 251s - loss: 0.3245 - accuracy: 0.8475 - val_loss: 0.6218 - val_accuracy: 0.7155\n",
      "Epoch 5/10\n",
      "327/327 - 249s - loss: 0.3076 - accuracy: 0.8631 - val_loss: 0.5616 - val_accuracy: 0.7131\n",
      "Epoch 6/10\n",
      "327/327 - 249s - loss: 0.2891 - accuracy: 0.8715 - val_loss: 0.5715 - val_accuracy: 0.7492\n",
      "Epoch 7/10\n",
      "327/327 - 250s - loss: 0.2789 - accuracy: 0.8793 - val_loss: 0.4240 - val_accuracy: 0.7989\n",
      "Epoch 8/10\n",
      "327/327 - 250s - loss: 0.2664 - accuracy: 0.8869 - val_loss: 0.4342 - val_accuracy: 0.7981\n",
      "Epoch 9/10\n",
      "327/327 - 251s - loss: 0.2653 - accuracy: 0.8912 - val_loss: 0.3876 - val_accuracy: 0.8237\n",
      "Epoch 10/10\n",
      "327/327 - 251s - loss: 0.2455 - accuracy: 0.8902 - val_loss: 0.5398 - val_accuracy: 0.7796\n"
     ]
    }
   ],
   "source": [
    "# Fit model on training data\n",
    "history = model2.fit_generator(generator = train_set,\n",
    "                              validation_data = test_set,\n",
    "                              epochs = 10,\n",
    "                              callbacks=[early_stopping_monitor],\n",
    "                              steps_per_epoch = train_size/batch_size, \n",
    "                              validation_steps = test_size/batch_size,\n",
    "                              shuffle = True,\n",
    "                              verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #3 - Pre-trained Model (VGG16 Convolutional Base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to use the artichecture of a pre-trained model as I was curious about transfer learning outcome. Transfer learning is using a pre-trained model and/or its weights on a different dataset. I chose to use only the arthitecture of VGG16 convolutional neural network model which is popular from ImageNet competition. This allowed me to save a lot of time testing the performance of my data with an additional model without requiring vast amount of hyperparameter tunning and optimizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = VGG16(weights = 'imagenet', \n",
    "#                  include_top = False,\n",
    "#                  input_shape = train_set.image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 Arthitecture\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape=train_set.image_shape, filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=4096,activation=\"relu\"))\n",
    "model.add(Dense(units=4096,activation=\"relu\"))\n",
    "model.add(Dense(units=2, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.001)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 134,268,738\n",
      "Trainable params: 134,268,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"vgg16.h5\", monitor='accuracy', verbose=1, save_best_only=True, \n",
    "                             save_weights_only=False, mode='max', save_freq=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\n",
      "Epoch 00001: accuracy improved from -inf to 0.62500, saving model to vgg16_1.h5\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (22.837382). Check your callbacks.\n",
      "  1/100 [..............................] - ETA: 1:03:23 - loss: 0.6930 - accuracy: 0.6250\n",
      "Epoch 00001: accuracy improved from 0.62500 to 0.65625, saving model to vgg16_1.h5\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (18.153782). Check your callbacks.\n",
      "  2/100 [..............................] - ETA: 52:05 - loss: 1.5380 - accuracy: 0.6562  \n",
      "Epoch 00001: accuracy did not improve from 0.65625\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (13.470182). Check your callbacks.\n",
      "  3/100 [..............................] - ETA: 41:10 - loss: 1.2569 - accuracy: 0.5417\n",
      "Epoch 00001: accuracy did not improve from 0.65625\n",
      "  4/100 [>.............................] - ETA: 36:02 - loss: 1.1149 - accuracy: 0.6094\n",
      "Epoch 00001: accuracy did not improve from 0.65625\n",
      "  5/100 [>.............................] - ETA: 32:48 - loss: 1.0250 - accuracy: 0.6500\n",
      "Epoch 00001: accuracy did not improve from 0.65625\n",
      "  6/100 [>.............................] - ETA: 30:32 - loss: 0.9774 - accuracy: 0.6354\n",
      "Epoch 00001: accuracy did not improve from 0.65625\n",
      "  7/100 [=>............................] - ETA: 28:51 - loss: 0.9287 - accuracy: 0.6518\n",
      "Epoch 00001: accuracy did not improve from 0.65625\n",
      "  8/100 [=>............................] - ETA: 27:35 - loss: 0.8946 - accuracy: 0.6562\n",
      "Epoch 00001: accuracy did not improve from 0.65625\n",
      "  9/100 [=>............................] - ETA: 26:33 - loss: 0.8714 - accuracy: 0.6458\n",
      "Epoch 00001: accuracy improved from 0.65625 to 0.66250, saving model to vgg16_1.h5\n",
      " 10/100 [==>...........................] - ETA: 27:19 - loss: 0.8463 - accuracy: 0.6625\n",
      "Epoch 00001: accuracy did not improve from 0.66250\n",
      " 11/100 [==>...........................] - ETA: 26:18 - loss: 0.8370 - accuracy: 0.6364\n",
      "Epoch 00001: accuracy did not improve from 0.66250\n",
      " 12/100 [==>...........................] - ETA: 25:31 - loss: 0.8198 - accuracy: 0.6458\n",
      "Epoch 00001: accuracy did not improve from 0.66250\n",
      " 13/100 [==>...........................] - ETA: 24:51 - loss: 0.8033 - accuracy: 0.6587\n",
      "Epoch 00001: accuracy did not improve from 0.66250\n",
      " 14/100 [===>..........................] - ETA: 24:11 - loss: 0.7916 - accuracy: 0.6607\n",
      "Epoch 00001: accuracy improved from 0.66250 to 0.67500, saving model to vgg16_1.h5\n",
      " 15/100 [===>..........................] - ETA: 24:11 - loss: 0.7757 - accuracy: 0.6750\n",
      "Epoch 00001: accuracy did not improve from 0.67500\n",
      " 16/100 [===>..........................] - ETA: 23:34 - loss: 0.7707 - accuracy: 0.6680\n",
      "Epoch 00001: accuracy did not improve from 0.67500\n",
      " 17/100 [====>.........................] - ETA: 23:01 - loss: 0.7643 - accuracy: 0.6654\n",
      "Epoch 00001: accuracy did not improve from 0.67500\n",
      " 18/100 [====>.........................] - ETA: 22:31 - loss: 0.7588 - accuracy: 0.6632\n",
      "Epoch 00001: accuracy did not improve from 0.67500\n",
      " 19/100 [====>.........................] - ETA: 22:03 - loss: 0.7493 - accuracy: 0.6678\n",
      "Epoch 00001: accuracy did not improve from 0.67500\n",
      " 20/100 [=====>........................] - ETA: 21:37 - loss: 0.7429 - accuracy: 0.6687\n",
      "Epoch 00001: accuracy did not improve from 0.67500\n",
      " 21/100 [=====>........................] - ETA: 21:12 - loss: 0.7345 - accuracy: 0.6726\n",
      "Epoch 00001: accuracy improved from 0.67500 to 0.67614, saving model to vgg16_1.h5\n",
      " 22/100 [=====>........................] - ETA: 21:01 - loss: 0.7267 - accuracy: 0.6761\n",
      "Epoch 00001: accuracy improved from 0.67614 to 0.68478, saving model to vgg16_1.h5\n",
      " 23/100 [=====>........................] - ETA: 20:50 - loss: 0.7127 - accuracy: 0.6848\n",
      "Epoch 00001: accuracy did not improve from 0.68478\n",
      " 24/100 [======>.......................] - ETA: 20:26 - loss: 0.7181 - accuracy: 0.6823\n",
      "Epoch 00001: accuracy improved from 0.68478 to 0.69500, saving model to vgg16_1.h5\n",
      " 25/100 [======>.......................] - ETA: 20:14 - loss: 0.6982 - accuracy: 0.6950\n",
      "Epoch 00001: accuracy did not improve from 0.69500\n",
      " 26/100 [======>.......................] - ETA: 19:51 - loss: 0.6998 - accuracy: 0.6923\n",
      "Epoch 00001: accuracy improved from 0.69500 to 0.69676, saving model to vgg16_1.h5\n",
      " 27/100 [=======>......................] - ETA: 19:37 - loss: 0.6920 - accuracy: 0.6968\n",
      "Epoch 00001: accuracy improved from 0.69676 to 0.70089, saving model to vgg16_1.h5\n",
      " 28/100 [=======>......................] - ETA: 19:23 - loss: 0.6850 - accuracy: 0.7009\n",
      "Epoch 00001: accuracy improved from 0.70089 to 0.70259, saving model to vgg16_1.h5\n",
      " 29/100 [=======>......................] - ETA: 19:12 - loss: 0.6807 - accuracy: 0.7026\n",
      "Epoch 00001: accuracy improved from 0.70259 to 0.70417, saving model to vgg16_1.h5\n",
      " 30/100 [========>.....................] - ETA: 18:59 - loss: 0.6768 - accuracy: 0.7042\n",
      "Epoch 00001: accuracy improved from 0.70417 to 0.70968, saving model to vgg16_1.h5\n",
      " 31/100 [========>.....................] - ETA: 18:45 - loss: 0.6692 - accuracy: 0.7097\n",
      "Epoch 00001: accuracy did not improve from 0.70968\n",
      " 32/100 [========>.....................] - ETA: 18:24 - loss: 0.6679 - accuracy: 0.7090\n",
      "Epoch 00001: accuracy did not improve from 0.70968\n",
      " 33/100 [========>.....................] - ETA: 18:04 - loss: 0.6667 - accuracy: 0.7083\n",
      "Epoch 00001: accuracy improved from 0.70968 to 0.71140, saving model to vgg16_1.h5\n",
      " 34/100 [=========>....................] - ETA: 17:51 - loss: 0.6617 - accuracy: 0.7114\n",
      "Epoch 00001: accuracy did not improve from 0.71140\n",
      " 35/100 [=========>....................] - ETA: 17:30 - loss: 0.6607 - accuracy: 0.7107\n",
      "Epoch 00001: accuracy improved from 0.71140 to 0.71181, saving model to vgg16_1.h5\n",
      " 36/100 [=========>....................] - ETA: 17:16 - loss: 0.6580 - accuracy: 0.7118\n",
      "Epoch 00001: accuracy improved from 0.71181 to 0.71453, saving model to vgg16_1.h5\n",
      " 37/100 [==========>...................] - ETA: 17:02 - loss: 0.6536 - accuracy: 0.7145\n",
      "Epoch 00001: accuracy did not improve from 0.71453\n",
      " 38/100 [==========>...................] - ETA: 16:42 - loss: 0.6530 - accuracy: 0.7138\n",
      "Epoch 00001: accuracy improved from 0.71453 to 0.72115, saving model to vgg16_1.h5\n",
      " 39/100 [==========>...................] - ETA: 16:27 - loss: 0.6437 - accuracy: 0.7212\n",
      "Epoch 00001: accuracy improved from 0.72115 to 0.72188, saving model to vgg16_1.h5\n",
      " 40/100 [===========>..................] - ETA: 16:13 - loss: 0.6417 - accuracy: 0.7219\n",
      "Epoch 00001: accuracy improved from 0.72188 to 0.72409, saving model to vgg16_1.h5\n",
      " 41/100 [===========>..................] - ETA: 15:58 - loss: 0.6378 - accuracy: 0.7241\n",
      "Epoch 00001: accuracy did not improve from 0.72409\n",
      " 42/100 [===========>..................] - ETA: 15:39 - loss: 0.6385 - accuracy: 0.7232\n",
      "Epoch 00001: accuracy improved from 0.72409 to 0.72529, saving model to vgg16_1.h5\n",
      " 43/100 [===========>..................] - ETA: 15:24 - loss: 0.6349 - accuracy: 0.7253\n",
      "Epoch 00001: accuracy improved from 0.72529 to 0.72585, saving model to vgg16_1.h5\n",
      " 44/100 [============>.................] - ETA: 15:10 - loss: 0.6335 - accuracy: 0.7259\n",
      "Epoch 00001: accuracy did not improve from 0.72585\n",
      " 45/100 [============>.................] - ETA: 14:51 - loss: 0.6359 - accuracy: 0.7236\n",
      "Epoch 00001: accuracy did not improve from 0.72585\n",
      " 46/100 [============>.................] - ETA: 14:32 - loss: 0.6343 - accuracy: 0.7242\n",
      "Epoch 00001: accuracy improved from 0.72585 to 0.72739, saving model to vgg16_1.h5\n",
      " 47/100 [=============>................] - ETA: 14:17 - loss: 0.6299 - accuracy: 0.7274\n",
      "Epoch 00001: accuracy improved from 0.72739 to 0.72786, saving model to vgg16_1.h5\n",
      " 48/100 [=============>................] - ETA: 14:02 - loss: 0.6285 - accuracy: 0.7279\n",
      "Epoch 00001: accuracy improved from 0.72786 to 0.72959, saving model to vgg16_1.h5\n",
      " 49/100 [=============>................] - ETA: 13:47 - loss: 0.6260 - accuracy: 0.7296\n",
      "Epoch 00001: accuracy did not improve from 0.72959\n",
      " 50/100 [==============>...............] - ETA: 13:28 - loss: 0.6271 - accuracy: 0.7275\n",
      "Epoch 00001: accuracy did not improve from 0.72959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 51/100 [==============>...............] - ETA: 13:10 - loss: 0.6259 - accuracy: 0.7279\n",
      "Epoch 00001: accuracy did not improve from 0.72959\n",
      " 52/100 [==============>...............] - ETA: 12:52 - loss: 0.6269 - accuracy: 0.7260\n",
      "Epoch 00001: accuracy did not improve from 0.72959\n",
      " 53/100 [==============>...............] - ETA: 12:34 - loss: 0.6248 - accuracy: 0.7276\n",
      "Epoch 00001: accuracy did not improve from 0.72959\n",
      " 54/100 [===============>..............] - ETA: 12:16 - loss: 0.6257 - accuracy: 0.7257\n",
      "Epoch 00001: accuracy did not improve from 0.72959\n",
      " 55/100 [===============>..............] - ETA: 11:58 - loss: 0.6238 - accuracy: 0.7273\n",
      "Epoch 00001: accuracy improved from 0.72959 to 0.73103, saving model to vgg16_1.h5\n",
      " 56/100 [===============>..............] - ETA: 11:43 - loss: 0.6200 - accuracy: 0.7310\n",
      "Epoch 00001: accuracy improved from 0.73103 to 0.73465, saving model to vgg16_1.h5\n",
      " 57/100 [================>.............] - ETA: 11:28 - loss: 0.6162 - accuracy: 0.7346\n",
      "Epoch 00001: accuracy improved from 0.73465 to 0.73491, saving model to vgg16_1.h5\n",
      " 58/100 [================>.............] - ETA: 11:13 - loss: 0.6154 - accuracy: 0.7349\n",
      "Epoch 00001: accuracy improved from 0.73491 to 0.73623, saving model to vgg16_1.h5\n",
      " 59/100 [================>.............] - ETA: 10:57 - loss: 0.6134 - accuracy: 0.7362\n",
      "Epoch 00001: accuracy improved from 0.73623 to 0.73646, saving model to vgg16_1.h5\n",
      " 60/100 [=================>............] - ETA: 10:42 - loss: 0.6125 - accuracy: 0.7365\n",
      "Epoch 00001: accuracy improved from 0.73646 to 0.73668, saving model to vgg16_1.h5\n",
      " 61/100 [=================>............] - ETA: 10:26 - loss: 0.6117 - accuracy: 0.7367\n",
      "Epoch 00001: accuracy improved from 0.73668 to 0.73891, saving model to vgg16_1.h5\n",
      " 62/100 [=================>............] - ETA: 10:11 - loss: 0.6083 - accuracy: 0.7389\n",
      "Epoch 00001: accuracy improved from 0.73891 to 0.73909, saving model to vgg16_1.h5\n",
      " 63/100 [=================>............] - ETA: 9:56 - loss: 0.6078 - accuracy: 0.7391 \n",
      "Epoch 00001: accuracy improved from 0.73909 to 0.74023, saving model to vgg16_1.h5\n",
      " 64/100 [==================>...........] - ETA: 9:40 - loss: 0.6059 - accuracy: 0.7402\n",
      "Epoch 00001: accuracy improved from 0.74023 to 0.74038, saving model to vgg16_1.h5\n",
      " 65/100 [==================>...........] - ETA: 9:25 - loss: 0.6057 - accuracy: 0.7404\n",
      "Epoch 00001: accuracy did not improve from 0.74038\n",
      " 66/100 [==================>...........] - ETA: 9:08 - loss: 0.6069 - accuracy: 0.7396\n",
      "Epoch 00001: accuracy did not improve from 0.74038\n",
      " 67/100 [===================>..........] - ETA: 8:50 - loss: 0.6064 - accuracy: 0.7397\n",
      "Epoch 00001: accuracy did not improve from 0.74038\n",
      " 68/100 [===================>..........] - ETA: 8:33 - loss: 0.6057 - accuracy: 0.7399\n",
      "Epoch 00001: accuracy improved from 0.74038 to 0.74275, saving model to vgg16_1.h5\n",
      " 69/100 [===================>..........] - ETA: 8:18 - loss: 0.6021 - accuracy: 0.7428\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 70/100 [====================>.........] - ETA: 8:01 - loss: 0.6025 - accuracy: 0.7420\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 71/100 [====================>.........] - ETA: 7:44 - loss: 0.6019 - accuracy: 0.7421\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 72/100 [====================>.........] - ETA: 7:27 - loss: 0.6031 - accuracy: 0.7405\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 73/100 [====================>.........] - ETA: 7:10 - loss: 0.6010 - accuracy: 0.7423\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 74/100 [=====================>........] - ETA: 6:54 - loss: 0.6020 - accuracy: 0.7407\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 75/100 [=====================>........] - ETA: 6:37 - loss: 0.6016 - accuracy: 0.7408\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 76/100 [=====================>........] - ETA: 6:20 - loss: 0.6025 - accuracy: 0.7393\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 77/100 [======================>.......] - ETA: 6:04 - loss: 0.6021 - accuracy: 0.7394\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 78/100 [======================>.......] - ETA: 5:47 - loss: 0.6030 - accuracy: 0.7380\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 79/100 [======================>.......] - ETA: 5:31 - loss: 0.6039 - accuracy: 0.7366\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 80/100 [=======================>......] - ETA: 5:14 - loss: 0.6041 - accuracy: 0.7359\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 81/100 [=======================>......] - ETA: 4:58 - loss: 0.6054 - accuracy: 0.7338\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 82/100 [=======================>......] - ETA: 4:42 - loss: 0.6056 - accuracy: 0.7332\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 83/100 [=======================>......] - ETA: 4:26 - loss: 0.6068 - accuracy: 0.7312\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 84/100 [========================>.....] - ETA: 4:10 - loss: 0.6057 - accuracy: 0.7329\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 85/100 [========================>.....] - ETA: 3:54 - loss: 0.6055 - accuracy: 0.7331\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 86/100 [========================>.....] - ETA: 3:38 - loss: 0.6057 - accuracy: 0.7326\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 87/100 [=========================>....] - ETA: 3:22 - loss: 0.6045 - accuracy: 0.7342\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 88/100 [=========================>....] - ETA: 3:06 - loss: 0.6057 - accuracy: 0.7322\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 89/100 [=========================>....] - ETA: 2:50 - loss: 0.6059 - accuracy: 0.7317\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 90/100 [==========================>...] - ETA: 2:35 - loss: 0.6050 - accuracy: 0.7326\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 91/100 [==========================>...] - ETA: 2:19 - loss: 0.6042 - accuracy: 0.7335\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 92/100 [==========================>...] - ETA: 2:03 - loss: 0.6038 - accuracy: 0.7337\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 93/100 [==========================>...] - ETA: 1:48 - loss: 0.6040 - accuracy: 0.7332\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 94/100 [===========================>..] - ETA: 1:32 - loss: 0.6053 - accuracy: 0.7314\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 95/100 [===========================>..] - ETA: 1:16 - loss: 0.6049 - accuracy: 0.7316\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 96/100 [===========================>..] - ETA: 1:01 - loss: 0.6045 - accuracy: 0.7318\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 97/100 [============================>.] - ETA: 46s - loss: 0.6053 - accuracy: 0.7307 \n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 98/100 [============================>.] - ETA: 30s - loss: 0.6049 - accuracy: 0.7309\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      " 99/100 [============================>.] - ETA: 15s - loss: 0.6039 - accuracy: 0.7317\n",
      "Epoch 00001: accuracy did not improve from 0.74275\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "100/100 [==============================] - 1568s 16s/step - loss: 0.6029 - accuracy: 0.7325 - val_loss: 0.6716 - val_accuracy: 0.6438\n",
      "Epoch 2/25\n",
      "\n",
      "Epoch 00002: accuracy did not improve from 0.74275\n",
      "  1/100 [..............................] - ETA: 22:21 - loss: 0.6268 - accuracy: 0.6875\n",
      "Epoch 00002: accuracy did not improve from 0.74275\n",
      "  2/100 [..............................] - ETA: 22:02 - loss: 0.6610 - accuracy: 0.6562\n",
      "Epoch 00002: accuracy did not improve from 0.74275\n",
      "  3/100 [..............................] - ETA: 21:40 - loss: 0.6062 - accuracy: 0.7083\n",
      "Epoch 00002: accuracy improved from 0.74275 to 0.75000, saving model to vgg16_1.h5\n",
      "  4/100 [>.............................] - ETA: 22:35 - loss: 0.5615 - accuracy: 0.7500\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      "  5/100 [>.............................] - ETA: 22:10 - loss: 0.5761 - accuracy: 0.7375\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6/100 [>.............................] - ETA: 21:50 - loss: 0.6107 - accuracy: 0.7083\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      "  7/100 [=>............................] - ETA: 21:29 - loss: 0.6138 - accuracy: 0.7054\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      "  8/100 [=>............................] - ETA: 21:10 - loss: 0.5992 - accuracy: 0.7188\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      "  9/100 [=>............................] - ETA: 20:52 - loss: 0.5952 - accuracy: 0.7222\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 10/100 [==>...........................] - ETA: 20:36 - loss: 0.5737 - accuracy: 0.7437\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 11/100 [==>...........................] - ETA: 20:23 - loss: 0.6015 - accuracy: 0.7159\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 12/100 [==>...........................] - ETA: 20:07 - loss: 0.6034 - accuracy: 0.7135\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 13/100 [==>...........................] - ETA: 19:50 - loss: 0.6005 - accuracy: 0.7163\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 14/100 [===>..........................] - ETA: 19:35 - loss: 0.5942 - accuracy: 0.7232\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 15/100 [===>..........................] - ETA: 19:20 - loss: 0.5816 - accuracy: 0.7375\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 16/100 [===>..........................] - ETA: 19:05 - loss: 0.5876 - accuracy: 0.7305\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 17/100 [====>.........................] - ETA: 18:49 - loss: 0.5897 - accuracy: 0.7279\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 18/100 [====>.........................] - ETA: 18:34 - loss: 0.5883 - accuracy: 0.7292\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 19/100 [====>.........................] - ETA: 18:19 - loss: 0.5871 - accuracy: 0.7303\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 20/100 [=====>........................] - ETA: 18:05 - loss: 0.5919 - accuracy: 0.7250\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 21/100 [=====>........................] - ETA: 17:51 - loss: 0.5906 - accuracy: 0.7262\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 22/100 [=====>........................] - ETA: 17:37 - loss: 0.5894 - accuracy: 0.7273\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 23/100 [=====>........................] - ETA: 17:23 - loss: 0.5883 - accuracy: 0.7283\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 24/100 [======>.......................] - ETA: 17:08 - loss: 0.5974 - accuracy: 0.7188\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 25/100 [======>.......................] - ETA: 16:54 - loss: 0.5937 - accuracy: 0.7225\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 26/100 [======>.......................] - ETA: 16:40 - loss: 0.5993 - accuracy: 0.7163\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 27/100 [=======>......................] - ETA: 16:26 - loss: 0.6002 - accuracy: 0.7153\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 28/100 [=======>......................] - ETA: 16:13 - loss: 0.6009 - accuracy: 0.7143\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 29/100 [=======>......................] - ETA: 15:59 - loss: 0.5998 - accuracy: 0.7155\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 30/100 [========>.....................] - ETA: 15:44 - loss: 0.5988 - accuracy: 0.7167\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 31/100 [========>.....................] - ETA: 15:31 - loss: 0.5963 - accuracy: 0.7198\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 32/100 [========>.....................] - ETA: 15:17 - loss: 0.6002 - accuracy: 0.7148\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 33/100 [========>.....................] - ETA: 15:03 - loss: 0.5994 - accuracy: 0.7159\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 34/100 [=========>....................] - ETA: 14:50 - loss: 0.5956 - accuracy: 0.7206\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 35/100 [=========>....................] - ETA: 14:37 - loss: 0.5920 - accuracy: 0.7250\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 36/100 [=========>....................] - ETA: 14:23 - loss: 0.5913 - accuracy: 0.7257\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 37/100 [==========>...................] - ETA: 14:09 - loss: 0.5951 - accuracy: 0.7213\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 38/100 [==========>...................] - ETA: 13:55 - loss: 0.5944 - accuracy: 0.7220\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 39/100 [==========>...................] - ETA: 13:42 - loss: 0.5936 - accuracy: 0.7228\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 40/100 [===========>..................] - ETA: 13:29 - loss: 0.5973 - accuracy: 0.7188\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 41/100 [===========>..................] - ETA: 13:16 - loss: 0.5951 - accuracy: 0.7210\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 42/100 [===========>..................] - ETA: 13:02 - loss: 0.5958 - accuracy: 0.7202\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 43/100 [===========>..................] - ETA: 12:49 - loss: 0.5923 - accuracy: 0.7238\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 44/100 [============>.................] - ETA: 12:35 - loss: 0.5902 - accuracy: 0.7259\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 45/100 [============>.................] - ETA: 12:22 - loss: 0.5896 - accuracy: 0.7264\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 46/100 [============>.................] - ETA: 12:08 - loss: 0.5905 - accuracy: 0.7255\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 47/100 [=============>................] - ETA: 11:54 - loss: 0.5914 - accuracy: 0.7247\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 48/100 [=============>................] - ETA: 11:41 - loss: 0.5879 - accuracy: 0.7279\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 49/100 [=============>................] - ETA: 11:27 - loss: 0.5903 - accuracy: 0.7258\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 50/100 [==============>...............] - ETA: 11:13 - loss: 0.5912 - accuracy: 0.7250\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 51/100 [==============>...............] - ETA: 11:00 - loss: 0.5892 - accuracy: 0.7267\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 52/100 [==============>...............] - ETA: 10:46 - loss: 0.5914 - accuracy: 0.7248\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 53/100 [==============>...............] - ETA: 10:32 - loss: 0.5909 - accuracy: 0.7252\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 54/100 [===============>..............] - ETA: 10:19 - loss: 0.5940 - accuracy: 0.7222\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 55/100 [===============>..............] - ETA: 10:05 - loss: 0.5890 - accuracy: 0.7273\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 56/100 [===============>..............] - ETA: 9:51 - loss: 0.5863 - accuracy: 0.7299 \n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 57/100 [================>.............] - ETA: 9:38 - loss: 0.5881 - accuracy: 0.7281\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 58/100 [================>.............] - ETA: 9:24 - loss: 0.5834 - accuracy: 0.7328\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 59/100 [================>.............] - ETA: 9:10 - loss: 0.5819 - accuracy: 0.7341\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 60/100 [=================>............] - ETA: 8:57 - loss: 0.5851 - accuracy: 0.7312\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 61/100 [=================>............] - ETA: 8:44 - loss: 0.5812 - accuracy: 0.7346\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 62/100 [=================>............] - ETA: 8:30 - loss: 0.5798 - accuracy: 0.7359\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 63/100 [=================>............] - ETA: 8:16 - loss: 0.5795 - accuracy: 0.7361\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 64/100 [==================>...........] - ETA: 8:03 - loss: 0.5780 - accuracy: 0.7373\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 65/100 [==================>...........] - ETA: 7:49 - loss: 0.5793 - accuracy: 0.7365\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 66/100 [==================>...........] - ETA: 7:36 - loss: 0.5791 - accuracy: 0.7367\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 67/100 [===================>..........] - ETA: 7:23 - loss: 0.5829 - accuracy: 0.7341\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 68/100 [===================>..........] - ETA: 7:09 - loss: 0.5837 - accuracy: 0.7335\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 69/100 [===================>..........] - ETA: 6:59 - loss: 0.5875 - accuracy: 0.7301\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 70/100 [====================>.........] - ETA: 7:08 - loss: 0.5871 - accuracy: 0.7304\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 71/100 [====================>.........] - ETA: 7:13 - loss: 0.5868 - accuracy: 0.7306\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 72/100 [====================>.........] - ETA: 7:17 - loss: 0.5887 - accuracy: 0.7283\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 73/100 [====================>.........] - ETA: 7:17 - loss: 0.5879 - accuracy: 0.7295\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 74/100 [=====================>........] - ETA: 7:16 - loss: 0.5890 - accuracy: 0.7280\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 75/100 [=====================>........] - ETA: 7:13 - loss: 0.5879 - accuracy: 0.7300\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 76/100 [=====================>........] - ETA: 7:08 - loss: 0.5879 - accuracy: 0.7303\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 77/100 [======================>.......] - ETA: 7:03 - loss: 0.5883 - accuracy: 0.7297\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 78/100 [======================>.......] - ETA: 6:55 - loss: 0.5888 - accuracy: 0.7292\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 79/100 [======================>.......] - ETA: 6:46 - loss: 0.5893 - accuracy: 0.7286\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 80/100 [=======================>......] - ETA: 6:37 - loss: 0.5887 - accuracy: 0.7297\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 81/100 [=======================>......] - ETA: 6:28 - loss: 0.5896 - accuracy: 0.7284\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 82/100 [=======================>......] - ETA: 6:15 - loss: 0.5886 - accuracy: 0.7302\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 83/100 [=======================>......] - ETA: 6:01 - loss: 0.5880 - accuracy: 0.7312\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 84/100 [========================>.....] - ETA: 5:47 - loss: 0.5879 - accuracy: 0.7314\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 85/100 [========================>.....] - ETA: 5:31 - loss: 0.5883 - accuracy: 0.7309\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 86/100 [========================>.....] - ETA: 5:15 - loss: 0.5887 - accuracy: 0.7304\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 87/100 [=========================>....] - ETA: 4:58 - loss: 0.5879 - accuracy: 0.7313\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 88/100 [=========================>....] - ETA: 4:40 - loss: 0.5872 - accuracy: 0.7322\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 89/100 [=========================>....] - ETA: 4:20 - loss: 0.5864 - accuracy: 0.7331\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 90/100 [==========================>...] - ETA: 4:00 - loss: 0.5855 - accuracy: 0.7340\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 91/100 [==========================>...] - ETA: 3:39 - loss: 0.5860 - accuracy: 0.7335\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 92/100 [==========================>...] - ETA: 3:18 - loss: 0.5872 - accuracy: 0.7323\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 93/100 [==========================>...] - ETA: 2:55 - loss: 0.5899 - accuracy: 0.7298\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 94/100 [===========================>..] - ETA: 2:32 - loss: 0.5903 - accuracy: 0.7294\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 95/100 [===========================>..] - ETA: 2:08 - loss: 0.5901 - accuracy: 0.7296\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 96/100 [===========================>..] - ETA: 1:44 - loss: 0.5910 - accuracy: 0.7285\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 97/100 [============================>.] - ETA: 1:19 - loss: 0.5908 - accuracy: 0.7287\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 98/100 [============================>.] - ETA: 53s - loss: 0.5889 - accuracy: 0.7309 \n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      " 99/100 [============================>.] - ETA: 27s - loss: 0.5892 - accuracy: 0.7304\n",
      "Epoch 00002: accuracy did not improve from 0.75000\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "100/100 [==============================] - 2910s 29s/step - loss: 0.5884 - accuracy: 0.7312 - val_loss: 0.6601 - val_accuracy: 0.6438\n",
      "Epoch 3/25\n",
      "\n",
      "Epoch 00003: accuracy did not improve from 0.75000\n",
      "  1/100 [..............................] - ETA: 1:31:30 - loss: 0.6219 - accuracy: 0.6875\n",
      "Epoch 00003: accuracy did not improve from 0.75000\n",
      "  2/100 [..............................] - ETA: 1:29:46 - loss: 0.5945 - accuracy: 0.7188\n",
      "Epoch 00003: accuracy did not improve from 0.75000\n",
      "  3/100 [..............................] - ETA: 1:29:19 - loss: 0.6221 - accuracy: 0.6875\n",
      "Epoch 00003: accuracy did not improve from 0.75000\n",
      "  4/100 [>.............................] - ETA: 1:29:13 - loss: 0.5945 - accuracy: 0.7188\n",
      "Epoch 00003: accuracy did not improve from 0.75000\n",
      "  5/100 [>.............................] - ETA: 1:28:45 - loss: 0.6112 - accuracy: 0.7000\n",
      "Epoch 00003: accuracy did not improve from 0.75000\n",
      "  6/100 [>.............................] - ETA: 1:27:41 - loss: 0.6223 - accuracy: 0.6875\n",
      "Epoch 00003: accuracy did not improve from 0.75000\n",
      "  7/100 [=>............................] - ETA: 1:26:18 - loss: 0.5986 - accuracy: 0.7143\n",
      "Epoch 00003: accuracy did not improve from 0.75000\n",
      "  8/100 [=>............................] - ETA: 1:25:16 - loss: 0.5737 - accuracy: 0.7422\n",
      "Epoch 00003: accuracy improved from 0.75000 to 0.75694, saving model to vgg16_1.h5\n",
      "  9/100 [=>............................] - ETA: 1:26:37 - loss: 0.5600 - accuracy: 0.7569\n",
      "Epoch 00003: accuracy improved from 0.75694 to 0.76875, saving model to vgg16_1.h5\n",
      " 10/100 [==>...........................] - ETA: 1:27:48 - loss: 0.5483 - accuracy: 0.7688\n",
      "Epoch 00003: accuracy did not improve from 0.76875\n",
      " 11/100 [==>...........................] - ETA: 1:26:26 - loss: 0.5496 - accuracy: 0.7670\n",
      "Epoch 00003: accuracy improved from 0.76875 to 0.77604, saving model to vgg16_1.h5\n",
      " 12/100 [==>...........................] - ETA: 1:26:32 - loss: 0.5393 - accuracy: 0.7760\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 13/100 [==>...........................] - ETA: 1:25:34 - loss: 0.5469 - accuracy: 0.7692\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 14/100 [===>..........................] - ETA: 1:24:31 - loss: 0.5652 - accuracy: 0.7545\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 15/100 [===>..........................] - ETA: 1:23:18 - loss: 0.5651 - accuracy: 0.7542\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 16/100 [===>..........................] - ETA: 1:22:06 - loss: 0.5602 - accuracy: 0.7578\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 17/100 [====>.........................] - ETA: 1:20:48 - loss: 0.5650 - accuracy: 0.7537\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 18/100 [====>.........................] - ETA: 1:19:38 - loss: 0.5565 - accuracy: 0.7604\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 19/100 [====>.........................] - ETA: 1:18:34 - loss: 0.5609 - accuracy: 0.7566\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 20/100 [=====>........................] - ETA: 1:17:35 - loss: 0.5573 - accuracy: 0.7594\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 21/100 [=====>........................] - ETA: 1:16:25 - loss: 0.5540 - accuracy: 0.7619\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22/100 [=====>........................] - ETA: 1:15:34 - loss: 0.5578 - accuracy: 0.7585\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 23/100 [=====>........................] - ETA: 1:14:24 - loss: 0.5612 - accuracy: 0.7554\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 24/100 [======>.......................] - ETA: 1:13:23 - loss: 0.5553 - accuracy: 0.7604\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 25/100 [======>.......................] - ETA: 1:12:26 - loss: 0.5585 - accuracy: 0.7575\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 26/100 [======>.......................] - ETA: 1:11:20 - loss: 0.5532 - accuracy: 0.7620\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 27/100 [=======>......................] - ETA: 1:10:15 - loss: 0.5614 - accuracy: 0.7546\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 28/100 [=======>......................] - ETA: 1:09:07 - loss: 0.5615 - accuracy: 0.7545\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 29/100 [=======>......................] - ETA: 1:08:05 - loss: 0.5662 - accuracy: 0.7500\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 30/100 [========>.....................] - ETA: 1:07:07 - loss: 0.5661 - accuracy: 0.7500\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 31/100 [========>.....................] - ETA: 1:06:13 - loss: 0.5639 - accuracy: 0.7520\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 32/100 [========>.....................] - ETA: 1:05:09 - loss: 0.5600 - accuracy: 0.7559\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 33/100 [========>.....................] - ETA: 1:04:09 - loss: 0.5639 - accuracy: 0.7519\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 34/100 [=========>....................] - ETA: 1:03:06 - loss: 0.5566 - accuracy: 0.7592\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 35/100 [=========>....................] - ETA: 1:02:06 - loss: 0.5532 - accuracy: 0.7625\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 36/100 [=========>....................] - ETA: 1:01:11 - loss: 0.5499 - accuracy: 0.7656\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 37/100 [==========>...................] - ETA: 1:00:11 - loss: 0.5502 - accuracy: 0.7652\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 38/100 [==========>...................] - ETA: 59:09 - loss: 0.5505 - accuracy: 0.7648  \n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 39/100 [==========>...................] - ETA: 58:14 - loss: 0.5527 - accuracy: 0.7628\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 40/100 [===========>..................] - ETA: 57:15 - loss: 0.5530 - accuracy: 0.7625\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 41/100 [===========>..................] - ETA: 56:21 - loss: 0.5514 - accuracy: 0.7637\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 42/100 [===========>..................] - ETA: 55:20 - loss: 0.5555 - accuracy: 0.7604\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 43/100 [===========>..................] - ETA: 54:21 - loss: 0.5520 - accuracy: 0.7631\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 44/100 [============>.................] - ETA: 53:20 - loss: 0.5559 - accuracy: 0.7599\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 45/100 [============>.................] - ETA: 52:20 - loss: 0.5561 - accuracy: 0.7597\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 46/100 [============>.................] - ETA: 51:21 - loss: 0.5546 - accuracy: 0.7609\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 47/100 [=============>................] - ETA: 50:26 - loss: 0.5595 - accuracy: 0.7566\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 48/100 [=============>................] - ETA: 49:28 - loss: 0.5625 - accuracy: 0.7539\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 49/100 [=============>................] - ETA: 48:28 - loss: 0.5625 - accuracy: 0.7538\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 50/100 [==============>...............] - ETA: 47:29 - loss: 0.5664 - accuracy: 0.7500\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 51/100 [==============>...............] - ETA: 46:29 - loss: 0.5616 - accuracy: 0.7549\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 52/100 [==============>...............] - ETA: 45:34 - loss: 0.5593 - accuracy: 0.7572\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 53/100 [==============>...............] - ETA: 44:35 - loss: 0.5583 - accuracy: 0.7583\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 54/100 [===============>..............] - ETA: 43:37 - loss: 0.5595 - accuracy: 0.7569\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 55/100 [===============>..............] - ETA: 42:37 - loss: 0.5618 - accuracy: 0.7545\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 56/100 [===============>..............] - ETA: 41:42 - loss: 0.5608 - accuracy: 0.7556\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 57/100 [================>.............] - ETA: 40:45 - loss: 0.5588 - accuracy: 0.7577\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 58/100 [================>.............] - ETA: 39:48 - loss: 0.5579 - accuracy: 0.7586\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 59/100 [================>.............] - ETA: 38:50 - loss: 0.5600 - accuracy: 0.7564\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 60/100 [=================>............] - ETA: 37:51 - loss: 0.5631 - accuracy: 0.7531\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 61/100 [=================>............] - ETA: 36:54 - loss: 0.5680 - accuracy: 0.7480\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 62/100 [=================>............] - ETA: 35:56 - loss: 0.5679 - accuracy: 0.7480\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 63/100 [=================>............] - ETA: 35:01 - loss: 0.5670 - accuracy: 0.7490\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 64/100 [==================>...........] - ETA: 34:03 - loss: 0.5670 - accuracy: 0.7490\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 65/100 [==================>...........] - ETA: 33:05 - loss: 0.5670 - accuracy: 0.7490\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 66/100 [==================>...........] - ETA: 32:07 - loss: 0.5670 - accuracy: 0.7491\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 67/100 [===================>..........] - ETA: 31:10 - loss: 0.5670 - accuracy: 0.7491\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 68/100 [===================>..........] - ETA: 30:14 - loss: 0.5661 - accuracy: 0.7500\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 69/100 [===================>..........] - ETA: 29:18 - loss: 0.5653 - accuracy: 0.7509\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 70/100 [====================>.........] - ETA: 28:20 - loss: 0.5661 - accuracy: 0.7500\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 71/100 [====================>.........] - ETA: 27:23 - loss: 0.5645 - accuracy: 0.7518\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 72/100 [====================>.........] - ETA: 26:26 - loss: 0.5637 - accuracy: 0.7526\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 73/100 [====================>.........] - ETA: 25:29 - loss: 0.5653 - accuracy: 0.7509\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 74/100 [=====================>........] - ETA: 24:33 - loss: 0.5645 - accuracy: 0.7517\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 75/100 [=====================>........] - ETA: 23:36 - loss: 0.5628 - accuracy: 0.7533\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 76/100 [=====================>........] - ETA: 22:39 - loss: 0.5628 - accuracy: 0.7533\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 77/100 [======================>.......] - ETA: 21:42 - loss: 0.5602 - accuracy: 0.7557\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 78/100 [======================>.......] - ETA: 20:46 - loss: 0.5593 - accuracy: 0.7564\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 79/100 [======================>.......] - ETA: 19:49 - loss: 0.5584 - accuracy: 0.7571\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 80/100 [=======================>......] - ETA: 18:52 - loss: 0.5575 - accuracy: 0.7578\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 81/100 [=======================>......] - ETA: 17:55 - loss: 0.5566 - accuracy: 0.7585\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 82/100 [=======================>......] - ETA: 16:58 - loss: 0.5568 - accuracy: 0.7584\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 83/100 [=======================>......] - ETA: 16:01 - loss: 0.5593 - accuracy: 0.7568\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 84/100 [========================>.....] - ETA: 15:05 - loss: 0.5606 - accuracy: 0.7560\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 85/100 [========================>.....] - ETA: 14:09 - loss: 0.5576 - accuracy: 0.7581\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 86/100 [========================>.....] - ETA: 13:12 - loss: 0.5568 - accuracy: 0.7587\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 87/100 [=========================>....] - ETA: 12:16 - loss: 0.5569 - accuracy: 0.7586\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 88/100 [=========================>....] - ETA: 11:20 - loss: 0.5599 - accuracy: 0.7564\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 89/100 [=========================>....] - ETA: 10:24 - loss: 0.5600 - accuracy: 0.7563\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 90/100 [==========================>...] - ETA: 9:27 - loss: 0.5617 - accuracy: 0.7549 \n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 91/100 [==========================>...] - ETA: 8:30 - loss: 0.5601 - accuracy: 0.7562\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 92/100 [==========================>...] - ETA: 7:34 - loss: 0.5586 - accuracy: 0.7575\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 93/100 [==========================>...] - ETA: 6:37 - loss: 0.5587 - accuracy: 0.7574\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 94/100 [===========================>..] - ETA: 5:40 - loss: 0.5609 - accuracy: 0.7553\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 95/100 [===========================>..] - ETA: 4:44 - loss: 0.5609 - accuracy: 0.7553\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 96/100 [===========================>..] - ETA: 3:47 - loss: 0.5623 - accuracy: 0.7539\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 97/100 [============================>.] - ETA: 2:50 - loss: 0.5610 - accuracy: 0.7552\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 98/100 [============================>.] - ETA: 1:53 - loss: 0.5604 - accuracy: 0.7557\n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      " 99/100 [============================>.] - ETA: 56s - loss: 0.5598 - accuracy: 0.7563 \n",
      "Epoch 00003: accuracy did not improve from 0.77604\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "100/100 [==============================] - 5869s 59s/step - loss: 0.5623 - accuracy: 0.7538 - val_loss: 0.6666 - val_accuracy: 0.6438\n",
      "Epoch 4/25\n",
      "\n",
      "Epoch 00004: accuracy improved from 0.77604 to 0.93750, saving model to vgg16_1.h5\n",
      "  1/100 [..............................] - ETA: 1:56:39 - loss: 0.3829 - accuracy: 0.9375\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      "  2/100 [..............................] - ETA: 1:46:11 - loss: 0.4132 - accuracy: 0.9062\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      "  3/100 [..............................] - ETA: 1:41:42 - loss: 0.5040 - accuracy: 0.8125\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      "  4/100 [>.............................] - ETA: 1:37:38 - loss: 0.5189 - accuracy: 0.7969\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      "  5/100 [>.............................] - ETA: 1:34:48 - loss: 0.5157 - accuracy: 0.8000\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      "  6/100 [>.............................] - ETA: 1:32:45 - loss: 0.5032 - accuracy: 0.8125\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      "  7/100 [=>............................] - ETA: 1:31:40 - loss: 0.5028 - accuracy: 0.8125\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      "  8/100 [=>............................] - ETA: 1:30:37 - loss: 0.5263 - accuracy: 0.7891\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      "  9/100 [=>............................] - ETA: 1:28:57 - loss: 0.5446 - accuracy: 0.7708\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 10/100 [==>...........................] - ETA: 1:27:39 - loss: 0.5400 - accuracy: 0.7750\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 11/100 [==>...........................] - ETA: 1:26:17 - loss: 0.5362 - accuracy: 0.7784\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 12/100 [==>...........................] - ETA: 1:25:22 - loss: 0.5219 - accuracy: 0.7917\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 13/100 [==>...........................] - ETA: 1:24:01 - loss: 0.5199 - accuracy: 0.7933\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 14/100 [===>..........................] - ETA: 1:23:01 - loss: 0.5180 - accuracy: 0.7946\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 15/100 [===>..........................] - ETA: 1:21:45 - loss: 0.5209 - accuracy: 0.7917\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 16/100 [===>..........................] - ETA: 1:20:53 - loss: 0.5465 - accuracy: 0.7695\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 17/100 [====>.........................] - ETA: 1:19:45 - loss: 0.5518 - accuracy: 0.7647\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 18/100 [====>.........................] - ETA: 1:18:31 - loss: 0.5444 - accuracy: 0.7708\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 19/100 [====>.........................] - ETA: 1:17:24 - loss: 0.5415 - accuracy: 0.7730\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 20/100 [=====>........................] - ETA: 1:16:09 - loss: 0.5462 - accuracy: 0.7688\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 21/100 [=====>........................] - ETA: 1:15:02 - loss: 0.5538 - accuracy: 0.7619\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 22/100 [=====>........................] - ETA: 1:13:51 - loss: 0.5542 - accuracy: 0.7614\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 23/100 [=====>........................] - ETA: 1:13:17 - loss: 0.5546 - accuracy: 0.7609\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 24/100 [======>.......................] - ETA: 1:12:14 - loss: 0.5520 - accuracy: 0.7630\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 25/100 [======>.......................] - ETA: 1:11:12 - loss: 0.5524 - accuracy: 0.7625\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 26/100 [======>.......................] - ETA: 1:10:03 - loss: 0.5502 - accuracy: 0.7644\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 27/100 [=======>......................] - ETA: 1:09:13 - loss: 0.5532 - accuracy: 0.7616\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 28/100 [=======>......................] - ETA: 1:08:11 - loss: 0.5535 - accuracy: 0.7612\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 29/100 [=======>......................] - ETA: 1:07:11 - loss: 0.5632 - accuracy: 0.7522\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 30/100 [========>.....................] - ETA: 1:06:14 - loss: 0.5565 - accuracy: 0.7583\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 31/100 [========>.....................] - ETA: 1:05:12 - loss: 0.5524 - accuracy: 0.7621\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 32/100 [========>.....................] - ETA: 1:04:15 - loss: 0.5527 - accuracy: 0.7617\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 33/100 [========>.....................] - ETA: 1:03:20 - loss: 0.5571 - accuracy: 0.7576\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 34/100 [=========>....................] - ETA: 1:02:21 - loss: 0.5553 - accuracy: 0.7592\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 35/100 [=========>....................] - ETA: 1:01:21 - loss: 0.5574 - accuracy: 0.7571\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 36/100 [=========>....................] - ETA: 1:00:27 - loss: 0.5557 - accuracy: 0.7587\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 37/100 [==========>...................] - ETA: 59:26 - loss: 0.5594 - accuracy: 0.7551  \n",
      "Epoch 00004: accuracy did not improve from 0.93750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38/100 [==========>...................] - ETA: 58:27 - loss: 0.5613 - accuracy: 0.7533\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 39/100 [==========>...................] - ETA: 57:30 - loss: 0.5579 - accuracy: 0.7564\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 40/100 [===========>..................] - ETA: 56:31 - loss: 0.5581 - accuracy: 0.7563\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 41/100 [===========>..................] - ETA: 55:33 - loss: 0.5598 - accuracy: 0.7546\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 42/100 [===========>..................] - ETA: 54:34 - loss: 0.5645 - accuracy: 0.7500\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 43/100 [===========>..................] - ETA: 53:36 - loss: 0.5615 - accuracy: 0.7529\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 44/100 [============>.................] - ETA: 52:39 - loss: 0.5615 - accuracy: 0.7528\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 45/100 [============>.................] - ETA: 51:40 - loss: 0.5615 - accuracy: 0.7528\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 46/100 [============>.................] - ETA: 50:43 - loss: 0.5602 - accuracy: 0.7541\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 47/100 [=============>................] - ETA: 49:47 - loss: 0.5602 - accuracy: 0.7540\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 48/100 [=============>................] - ETA: 48:47 - loss: 0.5630 - accuracy: 0.7513\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 49/100 [=============>................] - ETA: 47:51 - loss: 0.5616 - accuracy: 0.7526\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 50/100 [==============>...............] - ETA: 46:54 - loss: 0.5604 - accuracy: 0.7538\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 51/100 [==============>...............] - ETA: 45:57 - loss: 0.5604 - accuracy: 0.7537\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 52/100 [==============>...............] - ETA: 45:00 - loss: 0.5580 - accuracy: 0.7560\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 53/100 [==============>...............] - ETA: 44:01 - loss: 0.5580 - accuracy: 0.7559\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 54/100 [===============>..............] - ETA: 43:03 - loss: 0.5569 - accuracy: 0.7569\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 55/100 [===============>..............] - ETA: 42:07 - loss: 0.5533 - accuracy: 0.7602\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 56/100 [===============>..............] - ETA: 41:10 - loss: 0.5534 - accuracy: 0.7600\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 57/100 [================>.............] - ETA: 40:12 - loss: 0.5523 - accuracy: 0.7610\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 58/100 [================>.............] - ETA: 39:16 - loss: 0.5512 - accuracy: 0.7619\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 59/100 [================>.............] - ETA: 38:18 - loss: 0.5475 - accuracy: 0.7648\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 60/100 [=================>............] - ETA: 37:21 - loss: 0.5492 - accuracy: 0.7635\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 61/100 [=================>............] - ETA: 36:25 - loss: 0.5481 - accuracy: 0.7643\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 62/100 [=================>............] - ETA: 35:29 - loss: 0.5498 - accuracy: 0.7631\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 63/100 [=================>............] - ETA: 34:31 - loss: 0.5487 - accuracy: 0.7639\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 64/100 [==================>...........] - ETA: 33:35 - loss: 0.5491 - accuracy: 0.7637\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 65/100 [==================>...........] - ETA: 32:41 - loss: 0.5481 - accuracy: 0.7644\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 66/100 [==================>...........] - ETA: 31:45 - loss: 0.5471 - accuracy: 0.7652\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 67/100 [===================>..........] - ETA: 30:50 - loss: 0.5488 - accuracy: 0.7640\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 68/100 [===================>..........] - ETA: 29:53 - loss: 0.5504 - accuracy: 0.7629\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 69/100 [===================>..........] - ETA: 28:57 - loss: 0.5494 - accuracy: 0.7636\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 70/100 [====================>.........] - ETA: 28:01 - loss: 0.5497 - accuracy: 0.7634\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 71/100 [====================>.........] - ETA: 27:06 - loss: 0.5500 - accuracy: 0.7632\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 72/100 [====================>.........] - ETA: 26:09 - loss: 0.5524 - accuracy: 0.7613\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 73/100 [====================>.........] - ETA: 25:13 - loss: 0.5537 - accuracy: 0.7603\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 74/100 [=====================>........] - ETA: 24:17 - loss: 0.5538 - accuracy: 0.7601\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 75/100 [=====================>........] - ETA: 23:20 - loss: 0.5539 - accuracy: 0.7600\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 76/100 [=====================>........] - ETA: 22:24 - loss: 0.5531 - accuracy: 0.7607\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 77/100 [======================>.......] - ETA: 21:28 - loss: 0.5532 - accuracy: 0.7606\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 78/100 [======================>.......] - ETA: 20:31 - loss: 0.5525 - accuracy: 0.7612\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 79/100 [======================>.......] - ETA: 19:35 - loss: 0.5510 - accuracy: 0.7627\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 80/100 [=======================>......] - ETA: 18:39 - loss: 0.5511 - accuracy: 0.7625\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 81/100 [=======================>......] - ETA: 17:43 - loss: 0.5496 - accuracy: 0.7639\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 82/100 [=======================>......] - ETA: 16:47 - loss: 0.5514 - accuracy: 0.7622\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 83/100 [=======================>......] - ETA: 15:51 - loss: 0.5500 - accuracy: 0.7636\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 84/100 [========================>.....] - ETA: 14:55 - loss: 0.5501 - accuracy: 0.7634\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 85/100 [========================>.....] - ETA: 13:59 - loss: 0.5526 - accuracy: 0.7610\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 86/100 [========================>.....] - ETA: 13:03 - loss: 0.5504 - accuracy: 0.7631\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 87/100 [=========================>....] - ETA: 12:07 - loss: 0.5505 - accuracy: 0.7629\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 88/100 [=========================>....] - ETA: 11:11 - loss: 0.5499 - accuracy: 0.7635\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 89/100 [=========================>....] - ETA: 10:15 - loss: 0.5508 - accuracy: 0.7626\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 90/100 [==========================>...] - ETA: 9:19 - loss: 0.5502 - accuracy: 0.7632 \n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 91/100 [==========================>...] - ETA: 8:23 - loss: 0.5496 - accuracy: 0.7637\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 92/100 [==========================>...] - ETA: 7:27 - loss: 0.5512 - accuracy: 0.7622\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 93/100 [==========================>...] - ETA: 6:31 - loss: 0.5499 - accuracy: 0.7634\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 94/100 [===========================>..] - ETA: 5:35 - loss: 0.5492 - accuracy: 0.7640\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 95/100 [===========================>..] - ETA: 4:39 - loss: 0.5501 - accuracy: 0.7632\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 96/100 [===========================>..] - ETA: 3:43 - loss: 0.5503 - accuracy: 0.7630\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 97/100 [============================>.] - ETA: 2:47 - loss: 0.5519 - accuracy: 0.7616\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 98/100 [============================>.] - ETA: 1:52 - loss: 0.5542 - accuracy: 0.7596\n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      " 99/100 [============================>.] - ETA: 56s - loss: 0.5542 - accuracy: 0.7595 \n",
      "Epoch 00004: accuracy did not improve from 0.93750\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "100/100 [==============================] - 5777s 58s/step - loss: 0.5536 - accuracy: 0.7600 - val_loss: 0.6787 - val_accuracy: 0.6438\n",
      "Epoch 5/25\n",
      "\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      "  1/100 [..............................] - ETA: 1:36:28 - loss: 0.5623 - accuracy: 0.7500\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      "  2/100 [..............................] - ETA: 1:32:48 - loss: 0.5963 - accuracy: 0.7188\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      "  3/100 [..............................] - ETA: 1:30:12 - loss: 0.5403 - accuracy: 0.7708\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      "  4/100 [>.............................] - ETA: 1:29:57 - loss: 0.5625 - accuracy: 0.7500\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      "  5/100 [>.............................] - ETA: 1:31:18 - loss: 0.6024 - accuracy: 0.7125\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      "  6/100 [>.............................] - ETA: 1:32:55 - loss: 0.6066 - accuracy: 0.7083\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      "  7/100 [=>............................] - ETA: 1:31:26 - loss: 0.6279 - accuracy: 0.6875\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      "  8/100 [=>............................] - ETA: 1:29:40 - loss: 0.6198 - accuracy: 0.6953\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      "  9/100 [=>............................] - ETA: 1:28:10 - loss: 0.6340 - accuracy: 0.6806\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 10/100 [==>...........................] - ETA: 1:27:37 - loss: 0.6330 - accuracy: 0.6812\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 11/100 [==>...........................] - ETA: 1:26:26 - loss: 0.6373 - accuracy: 0.6761\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 12/100 [==>...........................] - ETA: 1:25:04 - loss: 0.6314 - accuracy: 0.6823\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 13/100 [==>...........................] - ETA: 1:23:45 - loss: 0.6180 - accuracy: 0.6971\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 14/100 [===>..........................] - ETA: 1:22:56 - loss: 0.6027 - accuracy: 0.7143\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 15/100 [===>..........................] - ETA: 1:21:57 - loss: 0.6004 - accuracy: 0.7167\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 16/100 [===>..........................] - ETA: 1:21:07 - loss: 0.5949 - accuracy: 0.7227\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 17/100 [====>.........................] - ETA: 1:20:01 - loss: 0.5837 - accuracy: 0.7353\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 18/100 [====>.........................] - ETA: 1:19:02 - loss: 0.5889 - accuracy: 0.7292\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 19/100 [====>.........................] - ETA: 1:17:47 - loss: 0.5848 - accuracy: 0.7336\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 20/100 [=====>........................] - ETA: 1:16:51 - loss: 0.5782 - accuracy: 0.7406\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 21/100 [=====>........................] - ETA: 1:15:50 - loss: 0.5720 - accuracy: 0.7470\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 22/100 [=====>........................] - ETA: 1:14:50 - loss: 0.5637 - accuracy: 0.7557\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 23/100 [=====>........................] - ETA: 1:13:42 - loss: 0.5584 - accuracy: 0.7609\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 24/100 [======>.......................] - ETA: 1:12:37 - loss: 0.5612 - accuracy: 0.7578\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 25/100 [======>.......................] - ETA: 1:11:43 - loss: 0.5639 - accuracy: 0.7550\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 26/100 [======>.......................] - ETA: 1:10:48 - loss: 0.5587 - accuracy: 0.7596\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 27/100 [=======>......................] - ETA: 1:09:43 - loss: 0.5511 - accuracy: 0.7662\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 28/100 [=======>......................] - ETA: 1:08:42 - loss: 0.5516 - accuracy: 0.7656\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 29/100 [=======>......................] - ETA: 1:07:46 - loss: 0.5545 - accuracy: 0.7629\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 30/100 [========>.....................] - ETA: 1:06:40 - loss: 0.5574 - accuracy: 0.7604\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 31/100 [========>.....................] - ETA: 1:05:45 - loss: 0.5576 - accuracy: 0.7601\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 32/100 [========>.....................] - ETA: 1:04:47 - loss: 0.5578 - accuracy: 0.7598\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 33/100 [========>.....................] - ETA: 1:03:45 - loss: 0.5484 - accuracy: 0.7670\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 34/100 [=========>....................] - ETA: 1:02:43 - loss: 0.5489 - accuracy: 0.7665\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 35/100 [=========>....................] - ETA: 1:01:43 - loss: 0.5494 - accuracy: 0.7661\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 36/100 [=========>....................] - ETA: 1:00:45 - loss: 0.5524 - accuracy: 0.7639\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 37/100 [==========>...................] - ETA: 59:50 - loss: 0.5505 - accuracy: 0.7652  \n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 38/100 [==========>...................] - ETA: 58:49 - loss: 0.5555 - accuracy: 0.7615\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 39/100 [==========>...................] - ETA: 57:51 - loss: 0.5645 - accuracy: 0.7548\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 40/100 [===========>..................] - ETA: 56:49 - loss: 0.5748 - accuracy: 0.7469\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 41/100 [===========>..................] - ETA: 55:48 - loss: 0.5708 - accuracy: 0.7500\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 42/100 [===========>..................] - ETA: 54:56 - loss: 0.5775 - accuracy: 0.7440\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 43/100 [===========>..................] - ETA: 53:59 - loss: 0.5787 - accuracy: 0.7427\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 44/100 [============>.................] - ETA: 53:00 - loss: 0.5843 - accuracy: 0.7372\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 45/100 [============>.................] - ETA: 52:00 - loss: 0.5852 - accuracy: 0.7361\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 46/100 [============>.................] - ETA: 51:05 - loss: 0.5860 - accuracy: 0.7351\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 47/100 [=============>................] - ETA: 50:09 - loss: 0.5868 - accuracy: 0.7340\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 48/100 [=============>................] - ETA: 49:14 - loss: 0.5886 - accuracy: 0.7318\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 49/100 [=============>................] - ETA: 48:18 - loss: 0.5872 - accuracy: 0.7334\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 50/100 [==============>...............] - ETA: 47:23 - loss: 0.5848 - accuracy: 0.7362\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 51/100 [==============>...............] - ETA: 46:24 - loss: 0.5865 - accuracy: 0.7341\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 52/100 [==============>...............] - ETA: 45:26 - loss: 0.5872 - accuracy: 0.7332\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 53/100 [==============>...............] - ETA: 44:37 - loss: 0.5852 - accuracy: 0.7358\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 54/100 [===============>..............] - ETA: 43:19 - loss: 0.5867 - accuracy: 0.7338\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 55/100 [===============>..............] - ETA: 41:47 - loss: 0.5856 - accuracy: 0.7352\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 56/100 [===============>..............] - ETA: 40:17 - loss: 0.5855 - accuracy: 0.7355\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 57/100 [================>.............] - ETA: 38:49 - loss: 0.5861 - accuracy: 0.7346\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 58/100 [================>.............] - ETA: 37:24 - loss: 0.5843 - accuracy: 0.7371\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 59/100 [================>.............] - ETA: 36:02 - loss: 0.5827 - accuracy: 0.7394\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 60/100 [=================>............] - ETA: 34:41 - loss: 0.5848 - accuracy: 0.7365\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 61/100 [=================>............] - ETA: 33:24 - loss: 0.5832 - accuracy: 0.7387\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 62/100 [=================>............] - ETA: 32:08 - loss: 0.5860 - accuracy: 0.7349\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 63/100 [=================>............] - ETA: 30:54 - loss: 0.5873 - accuracy: 0.7331\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 64/100 [==================>...........] - ETA: 29:43 - loss: 0.5857 - accuracy: 0.7354\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 65/100 [==================>...........] - ETA: 28:33 - loss: 0.5869 - accuracy: 0.7337\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 66/100 [==================>...........] - ETA: 27:25 - loss: 0.5875 - accuracy: 0.7330\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 67/100 [===================>..........] - ETA: 26:19 - loss: 0.5873 - accuracy: 0.7332\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 68/100 [===================>..........] - ETA: 25:14 - loss: 0.5863 - accuracy: 0.7344\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 69/100 [===================>..........] - ETA: 24:11 - loss: 0.5848 - accuracy: 0.7364\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 70/100 [====================>.........] - ETA: 23:09 - loss: 0.5846 - accuracy: 0.7366\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 71/100 [====================>.........] - ETA: 22:09 - loss: 0.5858 - accuracy: 0.7350\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 72/100 [====================>.........] - ETA: 21:10 - loss: 0.5863 - accuracy: 0.7344\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 73/100 [====================>.........] - ETA: 20:12 - loss: 0.5854 - accuracy: 0.7354\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 74/100 [=====================>........] - ETA: 19:16 - loss: 0.5845 - accuracy: 0.7365\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 75/100 [=====================>........] - ETA: 18:21 - loss: 0.5857 - accuracy: 0.7350\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 76/100 [=====================>........] - ETA: 17:27 - loss: 0.5841 - accuracy: 0.7368\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 77/100 [======================>.......] - ETA: 16:34 - loss: 0.5846 - accuracy: 0.7362\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 78/100 [======================>.......] - ETA: 15:42 - loss: 0.5851 - accuracy: 0.7356\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 79/100 [======================>.......] - ETA: 14:52 - loss: 0.5869 - accuracy: 0.7334\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 80/100 [=======================>......] - ETA: 14:02 - loss: 0.5860 - accuracy: 0.7344\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 81/100 [=======================>......] - ETA: 13:13 - loss: 0.5871 - accuracy: 0.7330\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 82/100 [=======================>......] - ETA: 12:25 - loss: 0.5855 - accuracy: 0.7348\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 83/100 [=======================>......] - ETA: 11:38 - loss: 0.5853 - accuracy: 0.7349\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 84/100 [========================>.....] - ETA: 10:51 - loss: 0.5850 - accuracy: 0.7351\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 85/100 [========================>.....] - ETA: 10:06 - loss: 0.5855 - accuracy: 0.7346\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 86/100 [========================>.....] - ETA: 9:21 - loss: 0.5833 - accuracy: 0.7369 \n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 87/100 [=========================>....] - ETA: 8:37 - loss: 0.5844 - accuracy: 0.7356\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 88/100 [=========================>....] - ETA: 7:53 - loss: 0.5828 - accuracy: 0.7372\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 89/100 [=========================>....] - ETA: 7:10 - loss: 0.5819 - accuracy: 0.7381\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 90/100 [==========================>...] - ETA: 6:28 - loss: 0.5824 - accuracy: 0.7375\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 91/100 [==========================>...] - ETA: 5:47 - loss: 0.5829 - accuracy: 0.7370\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 92/100 [==========================>...] - ETA: 5:06 - loss: 0.5847 - accuracy: 0.7351\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 93/100 [==========================>...] - ETA: 4:26 - loss: 0.5831 - accuracy: 0.7366\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 94/100 [===========================>..] - ETA: 3:46 - loss: 0.5816 - accuracy: 0.7380\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 95/100 [===========================>..] - ETA: 3:07 - loss: 0.5800 - accuracy: 0.7395\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 96/100 [===========================>..] - ETA: 2:29 - loss: 0.5778 - accuracy: 0.7415\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 97/100 [============================>.] - ETA: 1:51 - loss: 0.5790 - accuracy: 0.7403\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 98/100 [============================>.] - ETA: 1:13 - loss: 0.5788 - accuracy: 0.7404\n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      " 99/100 [============================>.] - ETA: 36s - loss: 0.5794 - accuracy: 0.7399 \n",
      "Epoch 00005: accuracy did not improve from 0.93750\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "100/100 [==============================] - 3667s 37s/step - loss: 0.5806 - accuracy: 0.7387 - val_loss: 0.6839 - val_accuracy: 0.6438\n",
      "Epoch 6/25\n",
      "\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      "  1/100 [..............................] - ETA: 21:52 - loss: 0.4197 - accuracy: 0.8750\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      "  2/100 [..............................] - ETA: 21:37 - loss: 0.4551 - accuracy: 0.8438\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      "  3/100 [..............................] - ETA: 21:14 - loss: 0.4910 - accuracy: 0.8125\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      "  4/100 [>.............................] - ETA: 20:58 - loss: 0.5090 - accuracy: 0.7969\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      "  5/100 [>.............................] - ETA: 20:48 - loss: 0.5347 - accuracy: 0.7750\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      "  6/100 [>.............................] - ETA: 20:36 - loss: 0.5146 - accuracy: 0.7917\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      "  7/100 [=>............................] - ETA: 20:24 - loss: 0.5216 - accuracy: 0.7857\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      "  8/100 [=>............................] - ETA: 20:12 - loss: 0.5552 - accuracy: 0.7578\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      "  9/100 [=>............................] - ETA: 19:54 - loss: 0.5727 - accuracy: 0.7431\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 10/100 [==>...........................] - ETA: 19:38 - loss: 0.6012 - accuracy: 0.7188\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11/100 [==>...........................] - ETA: 19:24 - loss: 0.5977 - accuracy: 0.7216\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 12/100 [==>...........................] - ETA: 19:10 - loss: 0.6063 - accuracy: 0.7135\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 13/100 [==>...........................] - ETA: 18:59 - loss: 0.6236 - accuracy: 0.6971\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 14/100 [===>..........................] - ETA: 18:46 - loss: 0.6239 - accuracy: 0.6964\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 15/100 [===>..........................] - ETA: 18:31 - loss: 0.6240 - accuracy: 0.6958\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 16/100 [===>..........................] - ETA: 18:18 - loss: 0.6165 - accuracy: 0.7031\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 17/100 [====>.........................] - ETA: 18:03 - loss: 0.6134 - accuracy: 0.7059\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 18/100 [====>.........................] - ETA: 17:50 - loss: 0.6108 - accuracy: 0.7083\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 19/100 [====>.........................] - ETA: 17:36 - loss: 0.6144 - accuracy: 0.7039\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 20/100 [=====>........................] - ETA: 17:23 - loss: 0.6092 - accuracy: 0.7094\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 21/100 [=====>........................] - ETA: 17:11 - loss: 0.6150 - accuracy: 0.7024\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 22/100 [=====>........................] - ETA: 17:00 - loss: 0.6129 - accuracy: 0.7045\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 23/100 [=====>........................] - ETA: 16:51 - loss: 0.6109 - accuracy: 0.7065\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 24/100 [======>.......................] - ETA: 16:37 - loss: 0.6070 - accuracy: 0.7109\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 25/100 [======>.......................] - ETA: 16:23 - loss: 0.6055 - accuracy: 0.7125\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 26/100 [======>.......................] - ETA: 16:09 - loss: 0.6000 - accuracy: 0.7188\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 27/100 [=======>......................] - ETA: 15:56 - loss: 0.6028 - accuracy: 0.7153\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 28/100 [=======>......................] - ETA: 15:43 - loss: 0.5960 - accuracy: 0.7232\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 29/100 [=======>......................] - ETA: 15:30 - loss: 0.5950 - accuracy: 0.7241\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 30/100 [========>.....................] - ETA: 15:18 - loss: 0.5994 - accuracy: 0.7188\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 31/100 [========>.....................] - ETA: 15:05 - loss: 0.5967 - accuracy: 0.7218\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 32/100 [========>.....................] - ETA: 14:51 - loss: 0.5925 - accuracy: 0.7266\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 33/100 [========>.....................] - ETA: 14:38 - loss: 0.5934 - accuracy: 0.7254\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 34/100 [=========>....................] - ETA: 14:24 - loss: 0.5942 - accuracy: 0.7243\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 35/100 [=========>....................] - ETA: 14:10 - loss: 0.5935 - accuracy: 0.7250\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 36/100 [=========>....................] - ETA: 13:58 - loss: 0.5958 - accuracy: 0.7222\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 37/100 [==========>...................] - ETA: 13:45 - loss: 0.5965 - accuracy: 0.7213\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 38/100 [==========>...................] - ETA: 13:32 - loss: 0.5972 - accuracy: 0.7204\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 39/100 [==========>...................] - ETA: 13:19 - loss: 0.5978 - accuracy: 0.7196\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 40/100 [===========>..................] - ETA: 13:06 - loss: 0.5942 - accuracy: 0.7234\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 41/100 [===========>..................] - ETA: 12:52 - loss: 0.5977 - accuracy: 0.7195\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 42/100 [===========>..................] - ETA: 12:38 - loss: 0.6009 - accuracy: 0.7158\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 43/100 [===========>..................] - ETA: 12:25 - loss: 0.6040 - accuracy: 0.7122\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 44/100 [============>.................] - ETA: 12:12 - loss: 0.6044 - accuracy: 0.7116\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 45/100 [============>.................] - ETA: 11:59 - loss: 0.6024 - accuracy: 0.7139\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 46/100 [============>.................] - ETA: 11:46 - loss: 0.5992 - accuracy: 0.7174\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 47/100 [=============>................] - ETA: 11:33 - loss: 0.5962 - accuracy: 0.7207\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 48/100 [=============>................] - ETA: 11:19 - loss: 0.5990 - accuracy: 0.7174\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 49/100 [=============>................] - ETA: 11:06 - loss: 0.6018 - accuracy: 0.7143\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 50/100 [==============>...............] - ETA: 10:53 - loss: 0.6000 - accuracy: 0.7163\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 51/100 [==============>...............] - ETA: 10:39 - loss: 0.6015 - accuracy: 0.7145\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 52/100 [==============>...............] - ETA: 10:26 - loss: 0.6040 - accuracy: 0.7115\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 53/100 [==============>...............] - ETA: 10:13 - loss: 0.6033 - accuracy: 0.7123\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 54/100 [===============>..............] - ETA: 10:00 - loss: 0.6006 - accuracy: 0.7153\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 55/100 [===============>..............] - ETA: 9:47 - loss: 0.6010 - accuracy: 0.7148 \n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 56/100 [===============>..............] - ETA: 9:33 - loss: 0.6004 - accuracy: 0.7154\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 57/100 [================>.............] - ETA: 9:20 - loss: 0.5969 - accuracy: 0.7193\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 58/100 [================>.............] - ETA: 9:07 - loss: 0.6021 - accuracy: 0.7134\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 59/100 [================>.............] - ETA: 8:54 - loss: 0.6006 - accuracy: 0.7150\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 60/100 [=================>............] - ETA: 8:41 - loss: 0.6046 - accuracy: 0.7104\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 61/100 [=================>............] - ETA: 8:28 - loss: 0.6031 - accuracy: 0.7121\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 62/100 [=================>............] - ETA: 8:15 - loss: 0.6016 - accuracy: 0.7137\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 63/100 [=================>............] - ETA: 8:02 - loss: 0.6011 - accuracy: 0.7143\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 64/100 [==================>...........] - ETA: 7:49 - loss: 0.5997 - accuracy: 0.7158\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 65/100 [==================>...........] - ETA: 7:36 - loss: 0.5975 - accuracy: 0.7183\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 66/100 [==================>...........] - ETA: 7:23 - loss: 0.5987 - accuracy: 0.7169\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 67/100 [===================>..........] - ETA: 7:10 - loss: 0.5966 - accuracy: 0.7192\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 68/100 [===================>..........] - ETA: 6:57 - loss: 0.5945 - accuracy: 0.7215\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 69/100 [===================>..........] - ETA: 6:44 - loss: 0.5974 - accuracy: 0.7183\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 70/100 [====================>.........] - ETA: 6:31 - loss: 0.5953 - accuracy: 0.7205\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 71/100 [====================>.........] - ETA: 6:17 - loss: 0.5965 - accuracy: 0.7192\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 72/100 [====================>.........] - ETA: 6:04 - loss: 0.5953 - accuracy: 0.7205\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 73/100 [====================>.........] - ETA: 5:51 - loss: 0.5932 - accuracy: 0.7226\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 74/100 [=====================>........] - ETA: 5:38 - loss: 0.5928 - accuracy: 0.7230\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 75/100 [=====================>........] - ETA: 5:25 - loss: 0.5925 - accuracy: 0.7233\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 76/100 [=====================>........] - ETA: 5:12 - loss: 0.5921 - accuracy: 0.7237\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 77/100 [======================>.......] - ETA: 4:59 - loss: 0.5933 - accuracy: 0.7224\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 78/100 [======================>.......] - ETA: 4:46 - loss: 0.5937 - accuracy: 0.7220\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 79/100 [======================>.......] - ETA: 4:33 - loss: 0.5910 - accuracy: 0.7247\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 80/100 [=======================>......] - ETA: 4:20 - loss: 0.5906 - accuracy: 0.7250\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 81/100 [=======================>......] - ETA: 4:07 - loss: 0.5927 - accuracy: 0.7230\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 82/100 [=======================>......] - ETA: 3:54 - loss: 0.5939 - accuracy: 0.7218\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 83/100 [=======================>......] - ETA: 3:41 - loss: 0.5935 - accuracy: 0.7221\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 84/100 [========================>.....] - ETA: 3:28 - loss: 0.5946 - accuracy: 0.7210\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 85/100 [========================>.....] - ETA: 3:15 - loss: 0.5958 - accuracy: 0.7199\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 86/100 [========================>.....] - ETA: 3:02 - loss: 0.5939 - accuracy: 0.7217\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 87/100 [=========================>....] - ETA: 2:49 - loss: 0.5928 - accuracy: 0.7227\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 88/100 [=========================>....] - ETA: 2:36 - loss: 0.5925 - accuracy: 0.7230\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 89/100 [=========================>....] - ETA: 2:23 - loss: 0.5936 - accuracy: 0.7219\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 90/100 [==========================>...] - ETA: 2:10 - loss: 0.5919 - accuracy: 0.7236\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 91/100 [==========================>...] - ETA: 1:57 - loss: 0.5888 - accuracy: 0.7266\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 92/100 [==========================>...] - ETA: 1:44 - loss: 0.5878 - accuracy: 0.7276\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 93/100 [==========================>...] - ETA: 1:31 - loss: 0.5889 - accuracy: 0.7265\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 94/100 [===========================>..] - ETA: 1:18 - loss: 0.5886 - accuracy: 0.7267\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 95/100 [===========================>..] - ETA: 1:05 - loss: 0.5897 - accuracy: 0.7257\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 96/100 [===========================>..] - ETA: 52s - loss: 0.5902 - accuracy: 0.7253 \n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 97/100 [============================>.] - ETA: 39s - loss: 0.5885 - accuracy: 0.7268\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 98/100 [============================>.] - ETA: 26s - loss: 0.5869 - accuracy: 0.7283\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      " 99/100 [============================>.] - ETA: 13s - loss: 0.5859 - accuracy: 0.7292\n",
      "Epoch 00006: accuracy did not improve from 0.93750\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "100/100 [==============================] - 1358s 14s/step - loss: 0.5857 - accuracy: 0.7294 - val_loss: 0.6800 - val_accuracy: 0.6438\n",
      "Epoch 7/25\n",
      "\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      "  1/100 [..............................] - ETA: 23:01 - loss: 0.4932 - accuracy: 0.8125\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      "  2/100 [..............................] - ETA: 22:06 - loss: 0.5278 - accuracy: 0.7812\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      "  3/100 [..............................] - ETA: 22:11 - loss: 0.5393 - accuracy: 0.7708\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      "  4/100 [>.............................] - ETA: 22:08 - loss: 0.5272 - accuracy: 0.7812\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      "  5/100 [>.............................] - ETA: 21:47 - loss: 0.5052 - accuracy: 0.8000\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      "  6/100 [>.............................] - ETA: 21:31 - loss: 0.5272 - accuracy: 0.7812\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      "  7/100 [=>............................] - ETA: 21:17 - loss: 0.5216 - accuracy: 0.7857\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      "  8/100 [=>............................] - ETA: 21:03 - loss: 0.5173 - accuracy: 0.7891\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      "  9/100 [=>............................] - ETA: 20:50 - loss: 0.5053 - accuracy: 0.7986\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 10/100 [==>...........................] - ETA: 20:32 - loss: 0.5192 - accuracy: 0.7875\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 11/100 [==>...........................] - ETA: 20:16 - loss: 0.5234 - accuracy: 0.7841\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 12/100 [==>...........................] - ETA: 19:59 - loss: 0.5202 - accuracy: 0.7865\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 13/100 [==>...........................] - ETA: 19:43 - loss: 0.5363 - accuracy: 0.7740\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 14/100 [===>..........................] - ETA: 19:30 - loss: 0.5326 - accuracy: 0.7768\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 15/100 [===>..........................] - ETA: 19:16 - loss: 0.5294 - accuracy: 0.7792\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 16/100 [===>..........................] - ETA: 19:01 - loss: 0.5266 - accuracy: 0.7812\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 17/100 [====>.........................] - ETA: 18:45 - loss: 0.5194 - accuracy: 0.7868\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 18/100 [====>.........................] - ETA: 18:30 - loss: 0.5311 - accuracy: 0.7778\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 19/100 [====>.........................] - ETA: 18:16 - loss: 0.5373 - accuracy: 0.7730\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 20/100 [=====>........................] - ETA: 18:03 - loss: 0.5347 - accuracy: 0.7750\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 21/100 [=====>........................] - ETA: 17:50 - loss: 0.5400 - accuracy: 0.7708\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 22/100 [=====>........................] - ETA: 17:37 - loss: 0.5447 - accuracy: 0.7670\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 23/100 [=====>........................] - ETA: 17:23 - loss: 0.5590 - accuracy: 0.7554\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 24/100 [======>.......................] - ETA: 17:09 - loss: 0.5654 - accuracy: 0.7500\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 25/100 [======>.......................] - ETA: 16:54 - loss: 0.5682 - accuracy: 0.7475\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 26/100 [======>.......................] - ETA: 16:39 - loss: 0.5706 - accuracy: 0.7452\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 27/100 [=======>......................] - ETA: 16:25 - loss: 0.5654 - accuracy: 0.7500\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28/100 [=======>......................] - ETA: 16:12 - loss: 0.5723 - accuracy: 0.7433\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 29/100 [=======>......................] - ETA: 15:59 - loss: 0.5698 - accuracy: 0.7457\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 30/100 [========>.....................] - ETA: 15:46 - loss: 0.5737 - accuracy: 0.7417\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 31/100 [========>.....................] - ETA: 15:33 - loss: 0.5714 - accuracy: 0.7440\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 32/100 [========>.....................] - ETA: 15:19 - loss: 0.5712 - accuracy: 0.7441\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 33/100 [========>.....................] - ETA: 15:05 - loss: 0.5746 - accuracy: 0.7405\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 34/100 [=========>....................] - ETA: 14:51 - loss: 0.5726 - accuracy: 0.7426\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 35/100 [=========>....................] - ETA: 14:37 - loss: 0.5708 - accuracy: 0.7446\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 36/100 [=========>....................] - ETA: 14:24 - loss: 0.5722 - accuracy: 0.7431\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 37/100 [==========>...................] - ETA: 14:10 - loss: 0.5690 - accuracy: 0.7466\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 38/100 [==========>...................] - ETA: 13:57 - loss: 0.5719 - accuracy: 0.7434\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 39/100 [==========>...................] - ETA: 13:43 - loss: 0.5703 - accuracy: 0.7452\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 40/100 [===========>..................] - ETA: 13:29 - loss: 0.5702 - accuracy: 0.7453\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 41/100 [===========>..................] - ETA: 13:15 - loss: 0.5742 - accuracy: 0.7409\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 42/100 [===========>..................] - ETA: 13:02 - loss: 0.5754 - accuracy: 0.7396\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 43/100 [===========>..................] - ETA: 12:48 - loss: 0.5726 - accuracy: 0.7427\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 44/100 [============>.................] - ETA: 12:35 - loss: 0.5737 - accuracy: 0.7415\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 45/100 [============>.................] - ETA: 12:21 - loss: 0.5773 - accuracy: 0.7375\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 46/100 [============>.................] - ETA: 12:08 - loss: 0.5758 - accuracy: 0.7391\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 47/100 [=============>................] - ETA: 11:54 - loss: 0.5756 - accuracy: 0.7394\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 48/100 [=============>................] - ETA: 11:41 - loss: 0.5743 - accuracy: 0.7409\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 49/100 [=============>................] - ETA: 11:27 - loss: 0.5730 - accuracy: 0.7423\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 50/100 [==============>...............] - ETA: 11:13 - loss: 0.5751 - accuracy: 0.7400\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 51/100 [==============>...............] - ETA: 10:59 - loss: 0.5738 - accuracy: 0.7414\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 52/100 [==============>...............] - ETA: 10:46 - loss: 0.5737 - accuracy: 0.7416\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 53/100 [==============>...............] - ETA: 10:33 - loss: 0.5692 - accuracy: 0.7465\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 54/100 [===============>..............] - ETA: 10:20 - loss: 0.5670 - accuracy: 0.7488\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 55/100 [===============>..............] - ETA: 10:06 - loss: 0.5680 - accuracy: 0.7477\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 56/100 [===============>..............] - ETA: 9:53 - loss: 0.5669 - accuracy: 0.7489 \n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 57/100 [================>.............] - ETA: 9:39 - loss: 0.5668 - accuracy: 0.7489\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 58/100 [================>.............] - ETA: 9:26 - loss: 0.5646 - accuracy: 0.7511\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 59/100 [================>.............] - ETA: 9:12 - loss: 0.5678 - accuracy: 0.7479\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 60/100 [=================>............] - ETA: 8:59 - loss: 0.5710 - accuracy: 0.7448\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 61/100 [=================>............] - ETA: 8:45 - loss: 0.5687 - accuracy: 0.7469\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 62/100 [=================>............] - ETA: 8:31 - loss: 0.5686 - accuracy: 0.7470\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 63/100 [=================>............] - ETA: 8:18 - loss: 0.5696 - accuracy: 0.7460\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 64/100 [==================>...........] - ETA: 8:04 - loss: 0.5664 - accuracy: 0.7490\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 65/100 [==================>...........] - ETA: 7:51 - loss: 0.5684 - accuracy: 0.7471\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 66/100 [==================>...........] - ETA: 7:37 - loss: 0.5734 - accuracy: 0.7424\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 67/100 [===================>..........] - ETA: 7:24 - loss: 0.5752 - accuracy: 0.7407\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 68/100 [===================>..........] - ETA: 7:10 - loss: 0.5760 - accuracy: 0.7399\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 69/100 [===================>..........] - ETA: 6:57 - loss: 0.5777 - accuracy: 0.7382\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 70/100 [====================>.........] - ETA: 6:43 - loss: 0.5747 - accuracy: 0.7411\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 71/100 [====================>.........] - ETA: 6:30 - loss: 0.5746 - accuracy: 0.7412\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 72/100 [====================>.........] - ETA: 6:16 - loss: 0.5753 - accuracy: 0.7405\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 73/100 [====================>.........] - ETA: 6:03 - loss: 0.5742 - accuracy: 0.7414\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 74/100 [=====================>........] - ETA: 5:50 - loss: 0.5733 - accuracy: 0.7424\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 75/100 [=====================>........] - ETA: 5:36 - loss: 0.5765 - accuracy: 0.7392\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 76/100 [=====================>........] - ETA: 5:23 - loss: 0.5779 - accuracy: 0.7377\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 77/100 [======================>.......] - ETA: 5:09 - loss: 0.5777 - accuracy: 0.7378\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 78/100 [======================>.......] - ETA: 4:56 - loss: 0.5799 - accuracy: 0.7356\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 79/100 [======================>.......] - ETA: 4:42 - loss: 0.5789 - accuracy: 0.7366\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 80/100 [=======================>......] - ETA: 4:29 - loss: 0.5795 - accuracy: 0.7359\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 81/100 [=======================>......] - ETA: 4:15 - loss: 0.5793 - accuracy: 0.7361\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 82/100 [=======================>......] - ETA: 4:02 - loss: 0.5791 - accuracy: 0.7363\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 83/100 [=======================>......] - ETA: 3:48 - loss: 0.5797 - accuracy: 0.7357\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 84/100 [========================>.....] - ETA: 3:35 - loss: 0.5795 - accuracy: 0.7359\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 85/100 [========================>.....] - ETA: 3:21 - loss: 0.5793 - accuracy: 0.7360\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 86/100 [========================>.....] - ETA: 3:08 - loss: 0.5792 - accuracy: 0.7362\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 87/100 [=========================>....] - ETA: 2:54 - loss: 0.5810 - accuracy: 0.7342\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 88/100 [=========================>....] - ETA: 2:41 - loss: 0.5802 - accuracy: 0.7351\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 89/100 [=========================>....] - ETA: 2:27 - loss: 0.5787 - accuracy: 0.7367\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 90/100 [==========================>...] - ETA: 2:14 - loss: 0.5786 - accuracy: 0.7368\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 91/100 [==========================>...] - ETA: 2:01 - loss: 0.5778 - accuracy: 0.7376\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 92/100 [==========================>...] - ETA: 1:47 - loss: 0.5789 - accuracy: 0.7364\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 93/100 [==========================>...] - ETA: 1:34 - loss: 0.5788 - accuracy: 0.7366\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 94/100 [===========================>..] - ETA: 1:20 - loss: 0.5768 - accuracy: 0.7387\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 95/100 [===========================>..] - ETA: 1:07 - loss: 0.5779 - accuracy: 0.7375\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 96/100 [===========================>..] - ETA: 53s - loss: 0.5771 - accuracy: 0.7383 \n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 97/100 [============================>.] - ETA: 40s - loss: 0.5788 - accuracy: 0.7365\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 98/100 [============================>.] - ETA: 26s - loss: 0.5799 - accuracy: 0.7353\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      " 99/100 [============================>.] - ETA: 13s - loss: 0.5797 - accuracy: 0.7355\n",
      "Epoch 00007: accuracy did not improve from 0.93750\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "100/100 [==============================] - 1385s 14s/step - loss: 0.5808 - accuracy: 0.7344 - val_loss: 0.6652 - val_accuracy: 0.6438\n",
      "Epoch 8/25\n",
      "\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      "  1/100 [..............................] - ETA: 21:43 - loss: 0.5053 - accuracy: 0.8125\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      "  2/100 [..............................] - ETA: 21:29 - loss: 0.5645 - accuracy: 0.7500\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      "  3/100 [..............................] - ETA: 21:17 - loss: 0.5645 - accuracy: 0.7500\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      "  4/100 [>.............................] - ETA: 21:13 - loss: 0.5793 - accuracy: 0.7344\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      "  5/100 [>.............................] - ETA: 21:02 - loss: 0.5764 - accuracy: 0.7375\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      "  6/100 [>.............................] - ETA: 20:55 - loss: 0.5843 - accuracy: 0.7292\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      "  7/100 [=>............................] - ETA: 20:45 - loss: 0.5730 - accuracy: 0.7411\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      "  8/100 [=>............................] - ETA: 20:32 - loss: 0.5572 - accuracy: 0.7578\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      "  9/100 [=>............................] - ETA: 20:19 - loss: 0.5580 - accuracy: 0.7569\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 10/100 [==>...........................] - ETA: 20:04 - loss: 0.5466 - accuracy: 0.7688\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 11/100 [==>...........................] - ETA: 19:50 - loss: 0.5426 - accuracy: 0.7727\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 12/100 [==>...........................] - ETA: 19:38 - loss: 0.5393 - accuracy: 0.7760\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 13/100 [==>...........................] - ETA: 19:26 - loss: 0.5411 - accuracy: 0.7740\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 14/100 [===>..........................] - ETA: 19:19 - loss: 0.5472 - accuracy: 0.7679\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 15/100 [===>..........................] - ETA: 19:05 - loss: 0.5525 - accuracy: 0.7625\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 16/100 [===>..........................] - ETA: 18:53 - loss: 0.5572 - accuracy: 0.7578\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 17/100 [====>.........................] - ETA: 18:38 - loss: 0.5537 - accuracy: 0.7610\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 18/100 [====>.........................] - ETA: 18:24 - loss: 0.5506 - accuracy: 0.7639\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 19/100 [====>.........................] - ETA: 18:11 - loss: 0.5547 - accuracy: 0.7599\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 20/100 [=====>........................] - ETA: 17:58 - loss: 0.5650 - accuracy: 0.7500\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 21/100 [=====>........................] - ETA: 17:45 - loss: 0.5649 - accuracy: 0.7500\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 22/100 [=====>........................] - ETA: 17:31 - loss: 0.5648 - accuracy: 0.7500\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 23/100 [=====>........................] - ETA: 17:17 - loss: 0.5647 - accuracy: 0.7500\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 24/100 [======>.......................] - ETA: 17:03 - loss: 0.5646 - accuracy: 0.7500\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 25/100 [======>.......................] - ETA: 16:50 - loss: 0.5645 - accuracy: 0.7500\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 26/100 [======>.......................] - ETA: 16:37 - loss: 0.5593 - accuracy: 0.7548\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 27/100 [=======>......................] - ETA: 16:24 - loss: 0.5545 - accuracy: 0.7593\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 28/100 [=======>......................] - ETA: 16:11 - loss: 0.5548 - accuracy: 0.7589\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 29/100 [=======>......................] - ETA: 15:57 - loss: 0.5551 - accuracy: 0.7586\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 30/100 [========>.....................] - ETA: 15:43 - loss: 0.5553 - accuracy: 0.7583\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 31/100 [========>.....................] - ETA: 15:29 - loss: 0.5555 - accuracy: 0.7581\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 32/100 [========>.....................] - ETA: 15:15 - loss: 0.5557 - accuracy: 0.7578\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 33/100 [========>.....................] - ETA: 15:02 - loss: 0.5581 - accuracy: 0.7557\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 34/100 [=========>....................] - ETA: 14:49 - loss: 0.5603 - accuracy: 0.7537\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 35/100 [=========>....................] - ETA: 14:35 - loss: 0.5583 - accuracy: 0.7554\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 36/100 [=========>....................] - ETA: 14:22 - loss: 0.5623 - accuracy: 0.7517\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 37/100 [==========>...................] - ETA: 14:08 - loss: 0.5548 - accuracy: 0.7584\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 38/100 [==========>...................] - ETA: 13:55 - loss: 0.5550 - accuracy: 0.7582\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 39/100 [==========>...................] - ETA: 13:41 - loss: 0.5625 - accuracy: 0.7516\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 40/100 [===========>..................] - ETA: 13:28 - loss: 0.5625 - accuracy: 0.7516\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 41/100 [===========>..................] - ETA: 13:14 - loss: 0.5642 - accuracy: 0.7500\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 42/100 [===========>..................] - ETA: 13:01 - loss: 0.5691 - accuracy: 0.7455\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 43/100 [===========>..................] - ETA: 12:48 - loss: 0.5658 - accuracy: 0.7485\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44/100 [============>.................] - ETA: 12:34 - loss: 0.5703 - accuracy: 0.7443\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 45/100 [============>.................] - ETA: 12:20 - loss: 0.5687 - accuracy: 0.7458\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 46/100 [============>.................] - ETA: 12:06 - loss: 0.5671 - accuracy: 0.7473\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 47/100 [=============>................] - ETA: 11:53 - loss: 0.5642 - accuracy: 0.7500\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 48/100 [=============>................] - ETA: 11:40 - loss: 0.5628 - accuracy: 0.7513\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 49/100 [=============>................] - ETA: 11:26 - loss: 0.5628 - accuracy: 0.7513\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 50/100 [==============>...............] - ETA: 11:12 - loss: 0.5654 - accuracy: 0.7487\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 51/100 [==============>...............] - ETA: 10:59 - loss: 0.5654 - accuracy: 0.7488\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 52/100 [==============>...............] - ETA: 10:45 - loss: 0.5653 - accuracy: 0.7488\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 53/100 [==============>...............] - ETA: 10:32 - loss: 0.5665 - accuracy: 0.7476\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 54/100 [===============>..............] - ETA: 10:19 - loss: 0.5652 - accuracy: 0.7488\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 55/100 [===============>..............] - ETA: 10:05 - loss: 0.5664 - accuracy: 0.7477\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 56/100 [===============>..............] - ETA: 9:51 - loss: 0.5674 - accuracy: 0.7467 \n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 57/100 [================>.............] - ETA: 9:38 - loss: 0.5674 - accuracy: 0.7467\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 58/100 [================>.............] - ETA: 9:24 - loss: 0.5673 - accuracy: 0.7468\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 59/100 [================>.............] - ETA: 9:11 - loss: 0.5672 - accuracy: 0.7468\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 60/100 [=================>............] - ETA: 8:57 - loss: 0.5693 - accuracy: 0.7448\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 61/100 [=================>............] - ETA: 8:44 - loss: 0.5713 - accuracy: 0.7428\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 62/100 [=================>............] - ETA: 8:31 - loss: 0.5752 - accuracy: 0.7389\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 63/100 [=================>............] - ETA: 8:17 - loss: 0.5760 - accuracy: 0.7381\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 64/100 [==================>...........] - ETA: 8:04 - loss: 0.5739 - accuracy: 0.7402\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 65/100 [==================>...........] - ETA: 7:50 - loss: 0.5737 - accuracy: 0.7404\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 66/100 [==================>...........] - ETA: 7:37 - loss: 0.5754 - accuracy: 0.7386\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 67/100 [===================>..........] - ETA: 7:23 - loss: 0.5779 - accuracy: 0.7360\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 68/100 [===================>..........] - ETA: 7:10 - loss: 0.5760 - accuracy: 0.7381\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 69/100 [===================>..........] - ETA: 6:56 - loss: 0.5741 - accuracy: 0.7400\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 70/100 [====================>.........] - ETA: 6:43 - loss: 0.5723 - accuracy: 0.7420\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 71/100 [====================>.........] - ETA: 6:29 - loss: 0.5739 - accuracy: 0.7403\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 72/100 [====================>.........] - ETA: 6:16 - loss: 0.5754 - accuracy: 0.7387\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 73/100 [====================>.........] - ETA: 6:02 - loss: 0.5752 - accuracy: 0.7389\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 74/100 [=====================>........] - ETA: 5:49 - loss: 0.5751 - accuracy: 0.7390\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 75/100 [=====================>........] - ETA: 5:35 - loss: 0.5750 - accuracy: 0.7392\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 76/100 [=====================>........] - ETA: 5:22 - loss: 0.5748 - accuracy: 0.7393\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 77/100 [======================>.......] - ETA: 5:09 - loss: 0.5747 - accuracy: 0.7394\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 78/100 [======================>.......] - ETA: 4:55 - loss: 0.5768 - accuracy: 0.7372\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 79/100 [======================>.......] - ETA: 4:42 - loss: 0.5752 - accuracy: 0.7389\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 80/100 [=======================>......] - ETA: 4:28 - loss: 0.5758 - accuracy: 0.7383\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 81/100 [=======================>......] - ETA: 4:15 - loss: 0.5764 - accuracy: 0.7377\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 82/100 [=======================>......] - ETA: 4:01 - loss: 0.5755 - accuracy: 0.7386\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 83/100 [=======================>......] - ETA: 3:48 - loss: 0.5761 - accuracy: 0.7380\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 84/100 [========================>.....] - ETA: 3:34 - loss: 0.5746 - accuracy: 0.7396\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 85/100 [========================>.....] - ETA: 3:21 - loss: 0.5745 - accuracy: 0.7397\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 86/100 [========================>.....] - ETA: 3:08 - loss: 0.5730 - accuracy: 0.7413\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 87/100 [=========================>....] - ETA: 2:54 - loss: 0.5749 - accuracy: 0.7392\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 88/100 [=========================>....] - ETA: 2:41 - loss: 0.5734 - accuracy: 0.7408\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 89/100 [=========================>....] - ETA: 2:27 - loss: 0.5727 - accuracy: 0.7416\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 90/100 [==========================>...] - ETA: 2:14 - loss: 0.5726 - accuracy: 0.7417\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 91/100 [==========================>...] - ETA: 2:00 - loss: 0.5711 - accuracy: 0.7431\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 92/100 [==========================>...] - ETA: 1:47 - loss: 0.5696 - accuracy: 0.7446\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 93/100 [==========================>...] - ETA: 1:33 - loss: 0.5723 - accuracy: 0.7419\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 94/100 [===========================>..] - ETA: 1:20 - loss: 0.5729 - accuracy: 0.7414\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 95/100 [===========================>..] - ETA: 1:07 - loss: 0.5721 - accuracy: 0.7421\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 96/100 [===========================>..] - ETA: 53s - loss: 0.5706 - accuracy: 0.7435 \n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 97/100 [============================>.] - ETA: 40s - loss: 0.5726 - accuracy: 0.7416\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 98/100 [============================>.] - ETA: 26s - loss: 0.5739 - accuracy: 0.7404\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      " 99/100 [============================>.] - ETA: 13s - loss: 0.5738 - accuracy: 0.7405\n",
      "Epoch 00008: accuracy did not improve from 0.93750\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "100/100 [==============================] - 1381s 14s/step - loss: 0.5743 - accuracy: 0.7400 - val_loss: 0.6752 - val_accuracy: 0.6438\n",
      "Epoch 9/25\n",
      "\n",
      "Epoch 00009: accuracy did not improve from 0.93750\n",
      "  1/100 [..............................] - ETA: 22:01 - loss: 0.3634 - accuracy: 0.9375\n",
      "Epoch 00009: accuracy improved from 0.93750 to 0.96875, saving model to vgg16_1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/100 [..............................] - ETA: 26:23 - loss: 0.3296 - accuracy: 0.9688\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      "  3/100 [..............................] - ETA: 24:24 - loss: 0.3847 - accuracy: 0.9167\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      "  4/100 [>.............................] - ETA: 23:30 - loss: 0.4119 - accuracy: 0.8906\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      "  5/100 [>.............................] - ETA: 22:51 - loss: 0.4420 - accuracy: 0.8625\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      "  6/100 [>.............................] - ETA: 22:24 - loss: 0.4383 - accuracy: 0.8646\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      "  7/100 [=>............................] - ETA: 22:01 - loss: 0.4665 - accuracy: 0.8393\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      "  8/100 [=>............................] - ETA: 21:37 - loss: 0.4785 - accuracy: 0.8281\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      "  9/100 [=>............................] - ETA: 21:16 - loss: 0.4796 - accuracy: 0.8264\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 10/100 [==>...........................] - ETA: 20:55 - loss: 0.4880 - accuracy: 0.8188\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 11/100 [==>...........................] - ETA: 20:37 - loss: 0.5019 - accuracy: 0.8068\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 12/100 [==>...........................] - ETA: 20:19 - loss: 0.5135 - accuracy: 0.7969\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 13/100 [==>...........................] - ETA: 20:04 - loss: 0.5293 - accuracy: 0.7837\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 14/100 [===>..........................] - ETA: 19:49 - loss: 0.5154 - accuracy: 0.7946\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 15/100 [===>..........................] - ETA: 19:33 - loss: 0.5237 - accuracy: 0.7875\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 16/100 [===>..........................] - ETA: 19:16 - loss: 0.5262 - accuracy: 0.7852\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 17/100 [====>.........................] - ETA: 18:59 - loss: 0.5284 - accuracy: 0.7831\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 18/100 [====>.........................] - ETA: 18:43 - loss: 0.5220 - accuracy: 0.7882\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 19/100 [====>.........................] - ETA: 18:29 - loss: 0.5281 - accuracy: 0.7829\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 20/100 [=====>........................] - ETA: 18:14 - loss: 0.5299 - accuracy: 0.7812\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 21/100 [=====>........................] - ETA: 18:02 - loss: 0.5314 - accuracy: 0.7798\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 22/100 [=====>........................] - ETA: 17:48 - loss: 0.5396 - accuracy: 0.7727\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 23/100 [=====>........................] - ETA: 17:35 - loss: 0.5470 - accuracy: 0.7663\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 24/100 [======>.......................] - ETA: 17:20 - loss: 0.5477 - accuracy: 0.7656\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 25/100 [======>.......................] - ETA: 17:05 - loss: 0.5511 - accuracy: 0.7625\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 26/100 [======>.......................] - ETA: 16:49 - loss: 0.5569 - accuracy: 0.7572\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 27/100 [=======>......................] - ETA: 16:34 - loss: 0.5546 - accuracy: 0.7593\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 28/100 [=======>......................] - ETA: 16:20 - loss: 0.5525 - accuracy: 0.7612\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 29/100 [=======>......................] - ETA: 16:06 - loss: 0.5505 - accuracy: 0.7629\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 30/100 [========>.....................] - ETA: 15:53 - loss: 0.5444 - accuracy: 0.7688\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 31/100 [========>.....................] - ETA: 15:40 - loss: 0.5428 - accuracy: 0.7702\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 32/100 [========>.....................] - ETA: 15:26 - loss: 0.5435 - accuracy: 0.7695\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 33/100 [========>.....................] - ETA: 15:12 - loss: 0.5480 - accuracy: 0.7652\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 34/100 [=========>....................] - ETA: 14:58 - loss: 0.5524 - accuracy: 0.7610\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 35/100 [=========>....................] - ETA: 14:43 - loss: 0.5451 - accuracy: 0.7679\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 36/100 [=========>....................] - ETA: 14:30 - loss: 0.5475 - accuracy: 0.7656\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 37/100 [==========>...................] - ETA: 14:16 - loss: 0.5479 - accuracy: 0.7652\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 38/100 [==========>...................] - ETA: 14:03 - loss: 0.5483 - accuracy: 0.7648\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 39/100 [==========>...................] - ETA: 13:49 - loss: 0.5503 - accuracy: 0.7628\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 40/100 [===========>..................] - ETA: 13:34 - loss: 0.5506 - accuracy: 0.7625\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 41/100 [===========>..................] - ETA: 13:21 - loss: 0.5525 - accuracy: 0.7607\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 42/100 [===========>..................] - ETA: 13:07 - loss: 0.5512 - accuracy: 0.7619\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 43/100 [===========>..................] - ETA: 12:54 - loss: 0.5515 - accuracy: 0.7616\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 44/100 [============>.................] - ETA: 12:40 - loss: 0.5547 - accuracy: 0.7585\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 45/100 [============>.................] - ETA: 12:26 - loss: 0.5564 - accuracy: 0.7569\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 46/100 [============>.................] - ETA: 12:12 - loss: 0.5565 - accuracy: 0.7568\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 47/100 [=============>................] - ETA: 11:58 - loss: 0.5552 - accuracy: 0.7580\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 48/100 [=============>................] - ETA: 11:45 - loss: 0.5554 - accuracy: 0.7578\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 49/100 [=============>................] - ETA: 11:31 - loss: 0.5542 - accuracy: 0.7589\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 50/100 [==============>...............] - ETA: 11:17 - loss: 0.5596 - accuracy: 0.7538\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 51/100 [==============>...............] - ETA: 11:04 - loss: 0.5609 - accuracy: 0.7525\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 52/100 [==============>...............] - ETA: 10:50 - loss: 0.5622 - accuracy: 0.7512\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 53/100 [==============>...............] - ETA: 10:36 - loss: 0.5586 - accuracy: 0.7547\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 54/100 [===============>..............] - ETA: 10:22 - loss: 0.5587 - accuracy: 0.7546\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 55/100 [===============>..............] - ETA: 10:09 - loss: 0.5576 - accuracy: 0.7557\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 56/100 [===============>..............] - ETA: 9:55 - loss: 0.5554 - accuracy: 0.7578 \n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 57/100 [================>.............] - ETA: 9:42 - loss: 0.5578 - accuracy: 0.7555\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 58/100 [================>.............] - ETA: 9:28 - loss: 0.5557 - accuracy: 0.7575\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 59/100 [================>.............] - ETA: 9:15 - loss: 0.5591 - accuracy: 0.7542\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 60/100 [=================>............] - ETA: 9:01 - loss: 0.5635 - accuracy: 0.7500\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 61/100 [=================>............] - ETA: 8:47 - loss: 0.5666 - accuracy: 0.7469\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 62/100 [=================>............] - ETA: 8:34 - loss: 0.5656 - accuracy: 0.7480\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 63/100 [=================>............] - ETA: 8:20 - loss: 0.5695 - accuracy: 0.7440\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 64/100 [==================>...........] - ETA: 8:06 - loss: 0.5704 - accuracy: 0.7432\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 65/100 [==================>...........] - ETA: 7:53 - loss: 0.5693 - accuracy: 0.7442\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 66/100 [==================>...........] - ETA: 7:39 - loss: 0.5729 - accuracy: 0.7405\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 67/100 [===================>..........] - ETA: 7:26 - loss: 0.5728 - accuracy: 0.7407\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 68/100 [===================>..........] - ETA: 7:12 - loss: 0.5735 - accuracy: 0.7399\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 69/100 [===================>..........] - ETA: 6:59 - loss: 0.5734 - accuracy: 0.7400\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 70/100 [====================>.........] - ETA: 6:45 - loss: 0.5733 - accuracy: 0.7402\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 71/100 [====================>.........] - ETA: 6:31 - loss: 0.5708 - accuracy: 0.7430\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 72/100 [====================>.........] - ETA: 6:18 - loss: 0.5715 - accuracy: 0.7422\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 73/100 [====================>.........] - ETA: 6:04 - loss: 0.5722 - accuracy: 0.7414\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 74/100 [=====================>........] - ETA: 5:51 - loss: 0.5721 - accuracy: 0.7416\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 75/100 [=====================>........] - ETA: 5:37 - loss: 0.5721 - accuracy: 0.7417\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 76/100 [=====================>........] - ETA: 5:24 - loss: 0.5720 - accuracy: 0.7418\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 77/100 [======================>.......] - ETA: 5:10 - loss: 0.5719 - accuracy: 0.7419\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 78/100 [======================>.......] - ETA: 4:57 - loss: 0.5711 - accuracy: 0.7428\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 79/100 [======================>.......] - ETA: 4:43 - loss: 0.5704 - accuracy: 0.7437\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 80/100 [=======================>......] - ETA: 4:30 - loss: 0.5710 - accuracy: 0.7430\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 81/100 [=======================>......] - ETA: 4:16 - loss: 0.5709 - accuracy: 0.7431\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 82/100 [=======================>......] - ETA: 4:03 - loss: 0.5695 - accuracy: 0.7447\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 83/100 [=======================>......] - ETA: 3:49 - loss: 0.5701 - accuracy: 0.7440\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 84/100 [========================>.....] - ETA: 3:36 - loss: 0.5679 - accuracy: 0.7463\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 85/100 [========================>.....] - ETA: 3:22 - loss: 0.5672 - accuracy: 0.7471\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 86/100 [========================>.....] - ETA: 3:08 - loss: 0.5679 - accuracy: 0.7464\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 87/100 [=========================>....] - ETA: 2:55 - loss: 0.5678 - accuracy: 0.7464\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 88/100 [=========================>....] - ETA: 2:42 - loss: 0.5699 - accuracy: 0.7443\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 89/100 [=========================>....] - ETA: 2:28 - loss: 0.5698 - accuracy: 0.7444\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 90/100 [==========================>...] - ETA: 2:14 - loss: 0.5683 - accuracy: 0.7458\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 91/100 [==========================>...] - ETA: 2:01 - loss: 0.5690 - accuracy: 0.7452\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 92/100 [==========================>...] - ETA: 1:47 - loss: 0.5696 - accuracy: 0.7446\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 93/100 [==========================>...] - ETA: 1:34 - loss: 0.5688 - accuracy: 0.7453\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 94/100 [===========================>..] - ETA: 1:20 - loss: 0.5688 - accuracy: 0.7453\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 95/100 [===========================>..] - ETA: 1:07 - loss: 0.5694 - accuracy: 0.7447\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 96/100 [===========================>..] - ETA: 53s - loss: 0.5693 - accuracy: 0.7448 \n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 97/100 [============================>.] - ETA: 40s - loss: 0.5679 - accuracy: 0.7461\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 98/100 [============================>.] - ETA: 26s - loss: 0.5692 - accuracy: 0.7449\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      " 99/100 [============================>.] - ETA: 13s - loss: 0.5678 - accuracy: 0.7462\n",
      "Epoch 00009: accuracy did not improve from 0.96875\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "100/100 [==============================] - 1388s 14s/step - loss: 0.5657 - accuracy: 0.7481 - val_loss: 0.6800 - val_accuracy: 0.6438\n",
      "Epoch 10/25\n",
      "\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      "  1/100 [..............................] - ETA: 21:38 - loss: 0.4931 - accuracy: 0.8125\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      "  2/100 [..............................] - ETA: 21:24 - loss: 0.5630 - accuracy: 0.7500\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      "  3/100 [..............................] - ETA: 21:13 - loss: 0.5867 - accuracy: 0.7292\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      "  4/100 [>.............................] - ETA: 21:10 - loss: 0.5807 - accuracy: 0.7344\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      "  5/100 [>.............................] - ETA: 21:02 - loss: 0.6061 - accuracy: 0.7125\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      "  6/100 [>.............................] - ETA: 20:58 - loss: 0.5747 - accuracy: 0.7396\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      "  7/100 [=>............................] - ETA: 20:46 - loss: 0.5834 - accuracy: 0.7321\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      "  8/100 [=>............................] - ETA: 20:32 - loss: 0.5899 - accuracy: 0.7266\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      "  9/100 [=>............................] - ETA: 20:21 - loss: 0.6030 - accuracy: 0.7153\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 10/100 [==>...........................] - ETA: 20:06 - loss: 0.6062 - accuracy: 0.7125\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 11/100 [==>...........................] - ETA: 19:50 - loss: 0.6151 - accuracy: 0.7045\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 12/100 [==>...........................] - ETA: 19:36 - loss: 0.5991 - accuracy: 0.7188\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 13/100 [==>...........................] - ETA: 19:23 - loss: 0.5910 - accuracy: 0.7260\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 14/100 [===>..........................] - ETA: 19:12 - loss: 0.5841 - accuracy: 0.7321\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 15/100 [===>..........................] - ETA: 18:58 - loss: 0.5735 - accuracy: 0.7417\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 16/100 [===>..........................] - ETA: 18:44 - loss: 0.5942 - accuracy: 0.7227\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 17/100 [====>.........................] - ETA: 18:38 - loss: 0.6002 - accuracy: 0.7169\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 18/100 [====>.........................] - ETA: 18:28 - loss: 0.6018 - accuracy: 0.7153\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19/100 [====>.........................] - ETA: 18:21 - loss: 0.6101 - accuracy: 0.7072\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 20/100 [=====>........................] - ETA: 18:59 - loss: 0.6109 - accuracy: 0.7063\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 21/100 [=====>........................] - ETA: 20:15 - loss: 0.6176 - accuracy: 0.6994\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 22/100 [=====>........................] - ETA: 23:24 - loss: 0.6096 - accuracy: 0.7074\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 23/100 [=====>........................] - ETA: 25:59 - loss: 0.6076 - accuracy: 0.7092\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 24/100 [======>.......................] - ETA: 27:58 - loss: 0.6008 - accuracy: 0.7161\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 25/100 [======>.......................] - ETA: 29:59 - loss: 0.5947 - accuracy: 0.7225\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 26/100 [======>.......................] - ETA: 31:50 - loss: 0.5912 - accuracy: 0.7260\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 27/100 [=======>......................] - ETA: 32:45 - loss: 0.5880 - accuracy: 0.7292\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 28/100 [=======>......................] - ETA: 33:32 - loss: 0.5914 - accuracy: 0.7254\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 29/100 [=======>......................] - ETA: 34:05 - loss: 0.5926 - accuracy: 0.7241\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 30/100 [========>.....................] - ETA: 34:40 - loss: 0.5916 - accuracy: 0.7250\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 31/100 [========>.....................] - ETA: 35:00 - loss: 0.5888 - accuracy: 0.7278\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 32/100 [========>.....................] - ETA: 35:21 - loss: 0.5899 - accuracy: 0.7266\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 33/100 [========>.....................] - ETA: 35:40 - loss: 0.5910 - accuracy: 0.7254\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 34/100 [=========>....................] - ETA: 35:48 - loss: 0.5902 - accuracy: 0.7261\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 35/100 [=========>....................] - ETA: 35:54 - loss: 0.5877 - accuracy: 0.7286\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 36/100 [=========>....................] - ETA: 35:55 - loss: 0.5871 - accuracy: 0.7292\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 37/100 [==========>...................] - ETA: 35:55 - loss: 0.5865 - accuracy: 0.7297\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 38/100 [==========>...................] - ETA: 35:54 - loss: 0.5843 - accuracy: 0.7319\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 39/100 [==========>...................] - ETA: 35:52 - loss: 0.5838 - accuracy: 0.7324\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 40/100 [===========>..................] - ETA: 35:43 - loss: 0.5832 - accuracy: 0.7328\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 41/100 [===========>..................] - ETA: 35:32 - loss: 0.5828 - accuracy: 0.7332\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 42/100 [===========>..................] - ETA: 35:18 - loss: 0.5823 - accuracy: 0.7336\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 43/100 [===========>..................] - ETA: 35:03 - loss: 0.5819 - accuracy: 0.7340\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 44/100 [============>.................] - ETA: 34:48 - loss: 0.5786 - accuracy: 0.7372\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 45/100 [============>.................] - ETA: 34:33 - loss: 0.5739 - accuracy: 0.7417\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 46/100 [============>.................] - ETA: 34:12 - loss: 0.5723 - accuracy: 0.7432\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 47/100 [=============>................] - ETA: 33:51 - loss: 0.5707 - accuracy: 0.7447\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 48/100 [=============>................] - ETA: 33:28 - loss: 0.5677 - accuracy: 0.7474\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 49/100 [=============>................] - ETA: 33:05 - loss: 0.5718 - accuracy: 0.7436\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 50/100 [==============>...............] - ETA: 32:41 - loss: 0.5688 - accuracy: 0.7462\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 51/100 [==============>...............] - ETA: 32:18 - loss: 0.5673 - accuracy: 0.7475\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 52/100 [==============>...............] - ETA: 31:50 - loss: 0.5686 - accuracy: 0.7464\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 53/100 [==============>...............] - ETA: 31:22 - loss: 0.5671 - accuracy: 0.7476\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 54/100 [===============>..............] - ETA: 30:54 - loss: 0.5656 - accuracy: 0.7488\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 55/100 [===============>..............] - ETA: 30:24 - loss: 0.5642 - accuracy: 0.7500\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 56/100 [===============>..............] - ETA: 29:56 - loss: 0.5656 - accuracy: 0.7489\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 57/100 [================>.............] - ETA: 29:24 - loss: 0.5656 - accuracy: 0.7489\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 58/100 [================>.............] - ETA: 28:51 - loss: 0.5656 - accuracy: 0.7489\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 59/100 [================>.............] - ETA: 28:19 - loss: 0.5669 - accuracy: 0.7479\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 60/100 [=================>............] - ETA: 27:56 - loss: 0.5669 - accuracy: 0.7479\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 61/100 [=================>............] - ETA: 27:16 - loss: 0.5642 - accuracy: 0.7500\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 62/100 [=================>............] - ETA: 26:19 - loss: 0.5643 - accuracy: 0.7500\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 63/100 [=================>............] - ETA: 25:24 - loss: 0.5630 - accuracy: 0.7510\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 64/100 [==================>...........] - ETA: 24:31 - loss: 0.5630 - accuracy: 0.7510\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 65/100 [==================>...........] - ETA: 23:38 - loss: 0.5631 - accuracy: 0.7510\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 66/100 [==================>...........] - ETA: 22:46 - loss: 0.5667 - accuracy: 0.7481\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 67/100 [===================>..........] - ETA: 21:55 - loss: 0.5655 - accuracy: 0.7491\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 68/100 [===================>..........] - ETA: 21:05 - loss: 0.5655 - accuracy: 0.7491\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 69/100 [===================>..........] - ETA: 20:15 - loss: 0.5655 - accuracy: 0.7491\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 70/100 [====================>.........] - ETA: 19:27 - loss: 0.5643 - accuracy: 0.7500\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 71/100 [====================>.........] - ETA: 18:40 - loss: 0.5612 - accuracy: 0.7526\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 72/100 [====================>.........] - ETA: 17:54 - loss: 0.5602 - accuracy: 0.7535\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 73/100 [====================>.........] - ETA: 17:08 - loss: 0.5592 - accuracy: 0.7543\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 74/100 [=====================>........] - ETA: 16:24 - loss: 0.5592 - accuracy: 0.7542\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 75/100 [=====================>........] - ETA: 15:39 - loss: 0.5613 - accuracy: 0.7525\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 76/100 [=====================>........] - ETA: 14:55 - loss: 0.5613 - accuracy: 0.7525\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 77/100 [======================>.......] - ETA: 14:11 - loss: 0.5614 - accuracy: 0.7524\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 78/100 [======================>.......] - ETA: 13:28 - loss: 0.5624 - accuracy: 0.7516\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 79/100 [======================>.......] - ETA: 12:46 - loss: 0.5614 - accuracy: 0.7524\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 80/100 [=======================>......] - ETA: 12:05 - loss: 0.5614 - accuracy: 0.7523\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 81/100 [=======================>......] - ETA: 11:25 - loss: 0.5624 - accuracy: 0.7515\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 82/100 [=======================>......] - ETA: 10:45 - loss: 0.5633 - accuracy: 0.7508\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 83/100 [=======================>......] - ETA: 10:05 - loss: 0.5633 - accuracy: 0.7508\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 84/100 [========================>.....] - ETA: 9:26 - loss: 0.5632 - accuracy: 0.7507 \n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 85/100 [========================>.....] - ETA: 8:48 - loss: 0.5649 - accuracy: 0.7493\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 86/100 [========================>.....] - ETA: 8:10 - loss: 0.5681 - accuracy: 0.7464\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 87/100 [=========================>....] - ETA: 7:33 - loss: 0.5703 - accuracy: 0.7443\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 88/100 [=========================>....] - ETA: 6:55 - loss: 0.5695 - accuracy: 0.7450\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 89/100 [=========================>....] - ETA: 6:19 - loss: 0.5694 - accuracy: 0.7451\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 90/100 [==========================>...] - ETA: 5:42 - loss: 0.5701 - accuracy: 0.7444\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 91/100 [==========================>...] - ETA: 5:06 - loss: 0.5700 - accuracy: 0.7445\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 92/100 [==========================>...] - ETA: 4:31 - loss: 0.5719 - accuracy: 0.7425\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 93/100 [==========================>...] - ETA: 3:55 - loss: 0.5731 - accuracy: 0.7413\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 94/100 [===========================>..] - ETA: 3:21 - loss: 0.5743 - accuracy: 0.7400\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 95/100 [===========================>..] - ETA: 2:46 - loss: 0.5730 - accuracy: 0.7414\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 96/100 [===========================>..] - ETA: 2:12 - loss: 0.5741 - accuracy: 0.7402\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 97/100 [============================>.] - ETA: 1:39 - loss: 0.5746 - accuracy: 0.7397\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 98/100 [============================>.] - ETA: 1:05 - loss: 0.5745 - accuracy: 0.7398\n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      " 99/100 [============================>.] - ETA: 32s - loss: 0.5750 - accuracy: 0.7393 \n",
      "Epoch 00010: accuracy did not improve from 0.96875\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "100/100 [==============================] - 3313s 33s/step - loss: 0.5749 - accuracy: 0.7394 - val_loss: 0.6592 - val_accuracy: 0.6438\n",
      "Epoch 11/25\n",
      "\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      "  1/100 [..............................] - ETA: 29:44 - loss: 0.5142 - accuracy: 0.8125\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      "  2/100 [..............................] - ETA: 29:04 - loss: 0.5679 - accuracy: 0.7500\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      "  3/100 [..............................] - ETA: 27:29 - loss: 0.5858 - accuracy: 0.7292\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      "  4/100 [>.............................] - ETA: 26:41 - loss: 0.5549 - accuracy: 0.7656\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      "  5/100 [>.............................] - ETA: 26:18 - loss: 0.5363 - accuracy: 0.7875\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      "  6/100 [>.............................] - ETA: 26:00 - loss: 0.5505 - accuracy: 0.7708\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      "  7/100 [=>............................] - ETA: 25:51 - loss: 0.5377 - accuracy: 0.7857\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      "  8/100 [=>............................] - ETA: 25:46 - loss: 0.5346 - accuracy: 0.7891\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      "  9/100 [=>............................] - ETA: 25:39 - loss: 0.5626 - accuracy: 0.7569\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 10/100 [==>...........................] - ETA: 25:36 - loss: 0.5740 - accuracy: 0.7437\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 11/100 [==>...........................] - ETA: 25:33 - loss: 0.5534 - accuracy: 0.7670\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 12/100 [==>...........................] - ETA: 25:24 - loss: 0.5545 - accuracy: 0.7656\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 13/100 [==>...........................] - ETA: 25:10 - loss: 0.5640 - accuracy: 0.7548\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 14/100 [===>..........................] - ETA: 25:04 - loss: 0.5561 - accuracy: 0.7634\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 15/100 [===>..........................] - ETA: 24:57 - loss: 0.5492 - accuracy: 0.7708\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 16/100 [===>..........................] - ETA: 24:49 - loss: 0.5467 - accuracy: 0.7734\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 17/100 [====>.........................] - ETA: 24:37 - loss: 0.5443 - accuracy: 0.7757\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 18/100 [====>.........................] - ETA: 24:26 - loss: 0.5454 - accuracy: 0.7743\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 19/100 [====>.........................] - ETA: 24:13 - loss: 0.5432 - accuracy: 0.7763\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 20/100 [=====>........................] - ETA: 23:56 - loss: 0.5473 - accuracy: 0.7719\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 21/100 [=====>........................] - ETA: 23:38 - loss: 0.5510 - accuracy: 0.7679\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 22/100 [=====>........................] - ETA: 23:24 - loss: 0.5487 - accuracy: 0.7699\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 23/100 [=====>........................] - ETA: 23:06 - loss: 0.5549 - accuracy: 0.7636\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 24/100 [======>.......................] - ETA: 22:45 - loss: 0.5552 - accuracy: 0.7630\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 25/100 [======>.......................] - ETA: 22:23 - loss: 0.5504 - accuracy: 0.7675\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 26/100 [======>.......................] - ETA: 22:04 - loss: 0.5483 - accuracy: 0.7692\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 27/100 [=======>......................] - ETA: 21:48 - loss: 0.5513 - accuracy: 0.7662\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 28/100 [=======>......................] - ETA: 21:32 - loss: 0.5517 - accuracy: 0.7656\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 29/100 [=======>......................] - ETA: 21:12 - loss: 0.5521 - accuracy: 0.7651\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 30/100 [========>.....................] - ETA: 20:57 - loss: 0.5501 - accuracy: 0.7667\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 31/100 [========>.....................] - ETA: 20:38 - loss: 0.5483 - accuracy: 0.7681\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 32/100 [========>.....................] - ETA: 20:20 - loss: 0.5487 - accuracy: 0.7676\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 33/100 [========>.....................] - ETA: 20:04 - loss: 0.5470 - accuracy: 0.7689\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 34/100 [=========>....................] - ETA: 19:47 - loss: 0.5411 - accuracy: 0.7739\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35/100 [=========>....................] - ETA: 19:25 - loss: 0.5480 - accuracy: 0.7679\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 36/100 [=========>....................] - ETA: 19:06 - loss: 0.5443 - accuracy: 0.7708\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 37/100 [==========>...................] - ETA: 18:51 - loss: 0.5448 - accuracy: 0.7703\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 38/100 [==========>...................] - ETA: 18:32 - loss: 0.5453 - accuracy: 0.7697\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 39/100 [==========>...................] - ETA: 18:14 - loss: 0.5498 - accuracy: 0.7660\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 40/100 [===========>..................] - ETA: 17:59 - loss: 0.5443 - accuracy: 0.7703\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 41/100 [===========>..................] - ETA: 17:43 - loss: 0.5410 - accuracy: 0.7729\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 42/100 [===========>..................] - ETA: 17:25 - loss: 0.5360 - accuracy: 0.7768\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 43/100 [===========>..................] - ETA: 17:09 - loss: 0.5311 - accuracy: 0.7805\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 44/100 [============>.................] - ETA: 16:52 - loss: 0.5281 - accuracy: 0.7827\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 45/100 [============>.................] - ETA: 16:36 - loss: 0.5329 - accuracy: 0.7792\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 46/100 [============>.................] - ETA: 16:20 - loss: 0.5337 - accuracy: 0.7785\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 47/100 [=============>................] - ETA: 16:04 - loss: 0.5345 - accuracy: 0.7779\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 48/100 [=============>................] - ETA: 15:47 - loss: 0.5353 - accuracy: 0.7773\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 49/100 [=============>................] - ETA: 15:29 - loss: 0.5342 - accuracy: 0.7781\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 50/100 [==============>...............] - ETA: 15:10 - loss: 0.5350 - accuracy: 0.7775\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 51/100 [==============>...............] - ETA: 14:50 - loss: 0.5374 - accuracy: 0.7757\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 52/100 [==============>...............] - ETA: 14:32 - loss: 0.5398 - accuracy: 0.7740\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 53/100 [==============>...............] - ETA: 14:14 - loss: 0.5419 - accuracy: 0.7724\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 54/100 [===============>..............] - ETA: 13:57 - loss: 0.5424 - accuracy: 0.7720\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 55/100 [===============>..............] - ETA: 13:40 - loss: 0.5399 - accuracy: 0.7739\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 56/100 [===============>..............] - ETA: 13:23 - loss: 0.5418 - accuracy: 0.7723\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 57/100 [================>.............] - ETA: 13:06 - loss: 0.5435 - accuracy: 0.7708\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 58/100 [================>.............] - ETA: 12:49 - loss: 0.5439 - accuracy: 0.7705\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 59/100 [================>.............] - ETA: 12:32 - loss: 0.5442 - accuracy: 0.7701\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 60/100 [=================>............] - ETA: 12:15 - loss: 0.5409 - accuracy: 0.7729\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 61/100 [=================>............] - ETA: 11:57 - loss: 0.5424 - accuracy: 0.7715\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 62/100 [=================>............] - ETA: 11:40 - loss: 0.5462 - accuracy: 0.7681\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 63/100 [=================>............] - ETA: 11:22 - loss: 0.5464 - accuracy: 0.7679\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 64/100 [==================>...........] - ETA: 11:05 - loss: 0.5510 - accuracy: 0.7637\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 65/100 [==================>...........] - ETA: 10:47 - loss: 0.5512 - accuracy: 0.7635\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 66/100 [==================>...........] - ETA: 10:29 - loss: 0.5504 - accuracy: 0.7642\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 67/100 [===================>..........] - ETA: 10:11 - loss: 0.5505 - accuracy: 0.7640\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 68/100 [===================>..........] - ETA: 9:53 - loss: 0.5498 - accuracy: 0.7647 \n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 69/100 [===================>..........] - ETA: 9:35 - loss: 0.5481 - accuracy: 0.7663\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 70/100 [====================>.........] - ETA: 9:17 - loss: 0.5466 - accuracy: 0.7679\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 71/100 [====================>.........] - ETA: 8:58 - loss: 0.5477 - accuracy: 0.7667\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 72/100 [====================>.........] - ETA: 8:40 - loss: 0.5453 - accuracy: 0.7691\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 73/100 [====================>.........] - ETA: 8:21 - loss: 0.5455 - accuracy: 0.7688\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 74/100 [=====================>........] - ETA: 8:03 - loss: 0.5458 - accuracy: 0.7686\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 75/100 [=====================>........] - ETA: 7:44 - loss: 0.5451 - accuracy: 0.7692\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 76/100 [=====================>........] - ETA: 7:26 - loss: 0.5462 - accuracy: 0.7681\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 77/100 [======================>.......] - ETA: 7:07 - loss: 0.5464 - accuracy: 0.7679\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 78/100 [======================>.......] - ETA: 6:49 - loss: 0.5450 - accuracy: 0.7692\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 79/100 [======================>.......] - ETA: 6:31 - loss: 0.5460 - accuracy: 0.7682\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 80/100 [=======================>......] - ETA: 6:12 - loss: 0.5462 - accuracy: 0.7680\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 81/100 [=======================>......] - ETA: 5:53 - loss: 0.5473 - accuracy: 0.7670\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 82/100 [=======================>......] - ETA: 5:35 - loss: 0.5491 - accuracy: 0.7652\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 83/100 [=======================>......] - ETA: 5:16 - loss: 0.5492 - accuracy: 0.7651\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 84/100 [========================>.....] - ETA: 4:58 - loss: 0.5478 - accuracy: 0.7664\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 85/100 [========================>.....] - ETA: 4:39 - loss: 0.5495 - accuracy: 0.7647\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 86/100 [========================>.....] - ETA: 4:20 - loss: 0.5505 - accuracy: 0.7638\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 87/100 [=========================>....] - ETA: 4:02 - loss: 0.5521 - accuracy: 0.7622\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 88/100 [=========================>....] - ETA: 3:43 - loss: 0.5522 - accuracy: 0.7621\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 89/100 [=========================>....] - ETA: 3:25 - loss: 0.5524 - accuracy: 0.7619\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 90/100 [==========================>...] - ETA: 3:06 - loss: 0.5525 - accuracy: 0.7618\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 91/100 [==========================>...] - ETA: 2:47 - loss: 0.5519 - accuracy: 0.7624\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 92/100 [==========================>...] - ETA: 2:29 - loss: 0.5520 - accuracy: 0.7622\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 93/100 [==========================>...] - ETA: 2:10 - loss: 0.5514 - accuracy: 0.7628\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 94/100 [===========================>..] - ETA: 1:51 - loss: 0.5522 - accuracy: 0.7620\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 95/100 [===========================>..] - ETA: 1:33 - loss: 0.5537 - accuracy: 0.7605\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 96/100 [===========================>..] - ETA: 1:14 - loss: 0.5538 - accuracy: 0.7604\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 97/100 [============================>.] - ETA: 55s - loss: 0.5539 - accuracy: 0.7603 \n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 98/100 [============================>.] - ETA: 37s - loss: 0.5553 - accuracy: 0.7589\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      " 99/100 [============================>.] - ETA: 18s - loss: 0.5567 - accuracy: 0.7576\n",
      "Epoch 00011: accuracy did not improve from 0.96875\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "100/100 [==============================] - 1923s 19s/step - loss: 0.5542 - accuracy: 0.7600 - val_loss: 0.6712 - val_accuracy: 0.6438\n",
      "Epoch 12/25\n",
      "\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      "  1/100 [..............................] - ETA: 32:17 - loss: 0.4356 - accuracy: 0.8750\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      "  2/100 [..............................] - ETA: 32:23 - loss: 0.5632 - accuracy: 0.7500\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      "  3/100 [..............................] - ETA: 29:55 - loss: 0.5844 - accuracy: 0.7292\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      "  4/100 [>.............................] - ETA: 28:29 - loss: 0.5790 - accuracy: 0.7344\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      "  5/100 [>.............................] - ETA: 28:13 - loss: 0.5374 - accuracy: 0.7750\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      "  6/100 [>.............................] - ETA: 27:16 - loss: 0.5094 - accuracy: 0.8021\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      "  7/100 [=>............................] - ETA: 28:01 - loss: 0.5170 - accuracy: 0.7946\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      "  8/100 [=>............................] - ETA: 29:23 - loss: 0.5310 - accuracy: 0.7812\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      "  9/100 [=>............................] - ETA: 30:33 - loss: 0.5345 - accuracy: 0.7778\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 10/100 [==>...........................] - ETA: 31:26 - loss: 0.5373 - accuracy: 0.7750\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 11/100 [==>...........................] - ETA: 31:21 - loss: 0.5334 - accuracy: 0.7784\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 12/100 [==>...........................] - ETA: 31:01 - loss: 0.5245 - accuracy: 0.7865\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 13/100 [==>...........................] - ETA: 30:06 - loss: 0.5115 - accuracy: 0.7981\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 14/100 [===>..........................] - ETA: 29:16 - loss: 0.5252 - accuracy: 0.7857\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 15/100 [===>..........................] - ETA: 28:44 - loss: 0.5229 - accuracy: 0.7875\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 16/100 [===>..........................] - ETA: 28:19 - loss: 0.5299 - accuracy: 0.7812\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 17/100 [====>.........................] - ETA: 27:57 - loss: 0.5276 - accuracy: 0.7831\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 18/100 [====>.........................] - ETA: 27:34 - loss: 0.5377 - accuracy: 0.7743\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 19/100 [====>.........................] - ETA: 27:11 - loss: 0.5467 - accuracy: 0.7664\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 20/100 [=====>........................] - ETA: 26:48 - loss: 0.5439 - accuracy: 0.7688\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 21/100 [=====>........................] - ETA: 26:23 - loss: 0.5552 - accuracy: 0.7589\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 22/100 [=====>........................] - ETA: 26:01 - loss: 0.5522 - accuracy: 0.7614\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 23/100 [=====>........................] - ETA: 25:39 - loss: 0.5621 - accuracy: 0.7527\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 24/100 [======>.......................] - ETA: 25:17 - loss: 0.5621 - accuracy: 0.7526\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 25/100 [======>.......................] - ETA: 24:57 - loss: 0.5537 - accuracy: 0.7600\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 26/100 [======>.......................] - ETA: 24:33 - loss: 0.5594 - accuracy: 0.7548\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 27/100 [=======>......................] - ETA: 24:06 - loss: 0.5621 - accuracy: 0.7523\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 28/100 [=======>......................] - ETA: 23:50 - loss: 0.5596 - accuracy: 0.7545\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 29/100 [=======>......................] - ETA: 23:27 - loss: 0.5527 - accuracy: 0.7608\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 30/100 [========>.....................] - ETA: 23:08 - loss: 0.5530 - accuracy: 0.7604\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 31/100 [========>.....................] - ETA: 22:48 - loss: 0.5577 - accuracy: 0.7560\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 32/100 [========>.....................] - ETA: 22:29 - loss: 0.5579 - accuracy: 0.7559\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 33/100 [========>.....................] - ETA: 22:09 - loss: 0.5580 - accuracy: 0.7557\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 34/100 [=========>....................] - ETA: 21:48 - loss: 0.5541 - accuracy: 0.7592\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 35/100 [=========>....................] - ETA: 21:25 - loss: 0.5563 - accuracy: 0.7571\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 36/100 [=========>....................] - ETA: 21:01 - loss: 0.5546 - accuracy: 0.7587\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 37/100 [==========>...................] - ETA: 20:39 - loss: 0.5585 - accuracy: 0.7551\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 38/100 [==========>...................] - ETA: 20:18 - loss: 0.5586 - accuracy: 0.7549\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 39/100 [==========>...................] - ETA: 20:09 - loss: 0.5604 - accuracy: 0.7532\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 40/100 [===========>..................] - ETA: 19:47 - loss: 0.5605 - accuracy: 0.7531\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 41/100 [===========>..................] - ETA: 19:21 - loss: 0.5605 - accuracy: 0.7530\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 42/100 [===========>..................] - ETA: 18:59 - loss: 0.5621 - accuracy: 0.7515\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 43/100 [===========>..................] - ETA: 18:39 - loss: 0.5637 - accuracy: 0.7500\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 44/100 [============>.................] - ETA: 18:17 - loss: 0.5622 - accuracy: 0.7514\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 45/100 [============>.................] - ETA: 17:57 - loss: 0.5680 - accuracy: 0.7458\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 46/100 [============>.................] - ETA: 17:38 - loss: 0.5651 - accuracy: 0.7486\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 47/100 [=============>................] - ETA: 17:17 - loss: 0.5650 - accuracy: 0.7487\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 48/100 [=============>................] - ETA: 16:54 - loss: 0.5650 - accuracy: 0.7487\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 49/100 [=============>................] - ETA: 16:34 - loss: 0.5623 - accuracy: 0.7513\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 50/100 [==============>...............] - ETA: 16:19 - loss: 0.5597 - accuracy: 0.7538\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 51/100 [==============>...............] - ETA: 16:02 - loss: 0.5636 - accuracy: 0.7500\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 52/100 [==============>...............] - ETA: 15:42 - loss: 0.5648 - accuracy: 0.7488\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 53/100 [==============>...............] - ETA: 15:21 - loss: 0.5672 - accuracy: 0.7465\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 54/100 [===============>..............] - ETA: 15:00 - loss: 0.5719 - accuracy: 0.7419\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 55/100 [===============>..............] - ETA: 14:45 - loss: 0.5729 - accuracy: 0.7409\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 56/100 [===============>..............] - ETA: 14:23 - loss: 0.5716 - accuracy: 0.7422\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 57/100 [================>.............] - ETA: 14:02 - loss: 0.5715 - accuracy: 0.7423\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 58/100 [================>.............] - ETA: 13:42 - loss: 0.5724 - accuracy: 0.7414\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 59/100 [================>.............] - ETA: 13:21 - loss: 0.5691 - accuracy: 0.7447\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 60/100 [=================>............] - ETA: 13:01 - loss: 0.5711 - accuracy: 0.7427\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 61/100 [=================>............] - ETA: 12:41 - loss: 0.5710 - accuracy: 0.7428\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 62/100 [=================>............] - ETA: 12:21 - loss: 0.5708 - accuracy: 0.7429\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 63/100 [=================>............] - ETA: 12:00 - loss: 0.5698 - accuracy: 0.7440\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 64/100 [==================>...........] - ETA: 11:40 - loss: 0.5697 - accuracy: 0.7441\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 65/100 [==================>...........] - ETA: 11:20 - loss: 0.5696 - accuracy: 0.7442\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 66/100 [==================>...........] - ETA: 11:04 - loss: 0.5676 - accuracy: 0.7462\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 67/100 [===================>..........] - ETA: 10:44 - loss: 0.5694 - accuracy: 0.7444\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 68/100 [===================>..........] - ETA: 10:23 - loss: 0.5702 - accuracy: 0.7436\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 69/100 [===================>..........] - ETA: 10:03 - loss: 0.5710 - accuracy: 0.7428\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 70/100 [====================>.........] - ETA: 9:43 - loss: 0.5683 - accuracy: 0.7455 \n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 71/100 [====================>.........] - ETA: 9:24 - loss: 0.5691 - accuracy: 0.7447\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 72/100 [====================>.........] - ETA: 9:03 - loss: 0.5699 - accuracy: 0.7439\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 73/100 [====================>.........] - ETA: 8:44 - loss: 0.5740 - accuracy: 0.7397\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 74/100 [=====================>........] - ETA: 8:25 - loss: 0.5739 - accuracy: 0.7399\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 75/100 [=====================>........] - ETA: 8:05 - loss: 0.5746 - accuracy: 0.7392\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 76/100 [=====================>........] - ETA: 7:46 - loss: 0.5736 - accuracy: 0.7401\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 77/100 [======================>.......] - ETA: 7:27 - loss: 0.5759 - accuracy: 0.7378\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 78/100 [======================>.......] - ETA: 7:08 - loss: 0.5773 - accuracy: 0.7364\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 79/100 [======================>.......] - ETA: 6:49 - loss: 0.5756 - accuracy: 0.7381\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 80/100 [=======================>......] - ETA: 6:29 - loss: 0.5755 - accuracy: 0.7383\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 81/100 [=======================>......] - ETA: 6:10 - loss: 0.5753 - accuracy: 0.7384\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 82/100 [=======================>......] - ETA: 5:51 - loss: 0.5774 - accuracy: 0.7363\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 83/100 [=======================>......] - ETA: 5:32 - loss: 0.5779 - accuracy: 0.7357\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 84/100 [========================>.....] - ETA: 5:12 - loss: 0.5792 - accuracy: 0.7344\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 85/100 [========================>.....] - ETA: 4:52 - loss: 0.5790 - accuracy: 0.7346\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 86/100 [========================>.....] - ETA: 4:32 - loss: 0.5782 - accuracy: 0.7355\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 87/100 [=========================>....] - ETA: 4:14 - loss: 0.5787 - accuracy: 0.7349\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 88/100 [=========================>....] - ETA: 3:55 - loss: 0.5779 - accuracy: 0.7358\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 89/100 [=========================>....] - ETA: 3:35 - loss: 0.5797 - accuracy: 0.7338\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 90/100 [==========================>...] - ETA: 3:16 - loss: 0.5802 - accuracy: 0.7333\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 91/100 [==========================>...] - ETA: 2:57 - loss: 0.5800 - accuracy: 0.7335\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 92/100 [==========================>...] - ETA: 2:39 - loss: 0.5817 - accuracy: 0.7317\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 93/100 [==========================>...] - ETA: 2:19 - loss: 0.5834 - accuracy: 0.7298\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 94/100 [===========================>..] - ETA: 1:59 - loss: 0.5832 - accuracy: 0.7301\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 95/100 [===========================>..] - ETA: 1:39 - loss: 0.5818 - accuracy: 0.7316\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 96/100 [===========================>..] - ETA: 1:19 - loss: 0.5823 - accuracy: 0.7311\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 97/100 [============================>.] - ETA: 59s - loss: 0.5815 - accuracy: 0.7320 \n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 98/100 [============================>.] - ETA: 39s - loss: 0.5814 - accuracy: 0.7321\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      " 99/100 [============================>.] - ETA: 19s - loss: 0.5812 - accuracy: 0.7323\n",
      "Epoch 00012: accuracy did not improve from 0.96875\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "100/100 [==============================] - 2051s 21s/step - loss: 0.5805 - accuracy: 0.7331 - val_loss: 0.6613 - val_accuracy: 0.6438\n",
      "Epoch 13/25\n",
      "\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      "  1/100 [..............................] - ETA: 33:05 - loss: 0.4548 - accuracy: 0.8750\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      "  2/100 [..............................] - ETA: 32:26 - loss: 0.5105 - accuracy: 0.8125\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      "  3/100 [..............................] - ETA: 32:11 - loss: 0.5290 - accuracy: 0.7917\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      "  4/100 [>.............................] - ETA: 31:48 - loss: 0.5381 - accuracy: 0.7812\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      "  5/100 [>.............................] - ETA: 31:29 - loss: 0.5436 - accuracy: 0.7750\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      "  6/100 [>.............................] - ETA: 31:10 - loss: 0.5568 - accuracy: 0.7604\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      "  7/100 [=>............................] - ETA: 30:46 - loss: 0.5580 - accuracy: 0.7589\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      "  8/100 [=>............................] - ETA: 30:27 - loss: 0.5662 - accuracy: 0.7500\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9/100 [=>............................] - ETA: 30:07 - loss: 0.5463 - accuracy: 0.7708\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 10/100 [==>...........................] - ETA: 29:46 - loss: 0.5422 - accuracy: 0.7750\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 11/100 [==>...........................] - ETA: 29:27 - loss: 0.5661 - accuracy: 0.7500\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 12/100 [==>...........................] - ETA: 29:09 - loss: 0.5558 - accuracy: 0.7604\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 13/100 [==>...........................] - ETA: 28:48 - loss: 0.5470 - accuracy: 0.7692\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 14/100 [===>..........................] - ETA: 28:28 - loss: 0.5394 - accuracy: 0.7768\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 15/100 [===>..........................] - ETA: 28:08 - loss: 0.5410 - accuracy: 0.7750\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 16/100 [===>..........................] - ETA: 27:48 - loss: 0.5344 - accuracy: 0.7812\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 17/100 [====>.........................] - ETA: 27:27 - loss: 0.5361 - accuracy: 0.7794\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 18/100 [====>.........................] - ETA: 27:08 - loss: 0.5266 - accuracy: 0.7882\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 19/100 [====>.........................] - ETA: 26:48 - loss: 0.5285 - accuracy: 0.7862\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 20/100 [=====>........................] - ETA: 26:28 - loss: 0.5234 - accuracy: 0.7906\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 21/100 [=====>........................] - ETA: 26:08 - loss: 0.5219 - accuracy: 0.7917\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 22/100 [=====>........................] - ETA: 25:49 - loss: 0.5238 - accuracy: 0.7898\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 23/100 [=====>........................] - ETA: 25:28 - loss: 0.5255 - accuracy: 0.7880\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 24/100 [======>.......................] - ETA: 25:09 - loss: 0.5209 - accuracy: 0.7917\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 25/100 [======>.......................] - ETA: 24:49 - loss: 0.5347 - accuracy: 0.7800\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 26/100 [======>.......................] - ETA: 24:29 - loss: 0.5270 - accuracy: 0.7861\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 27/100 [=======>......................] - ETA: 24:09 - loss: 0.5370 - accuracy: 0.7778\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 28/100 [=======>......................] - ETA: 23:49 - loss: 0.5407 - accuracy: 0.7746\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 29/100 [=======>......................] - ETA: 23:29 - loss: 0.5389 - accuracy: 0.7759\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 30/100 [========>.....................] - ETA: 23:09 - loss: 0.5423 - accuracy: 0.7729\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 31/100 [========>.....................] - ETA: 22:49 - loss: 0.5430 - accuracy: 0.7722\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 32/100 [========>.....................] - ETA: 22:29 - loss: 0.5460 - accuracy: 0.7695\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 33/100 [========>.....................] - ETA: 22:09 - loss: 0.5442 - accuracy: 0.7708\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 34/100 [=========>....................] - ETA: 21:49 - loss: 0.5448 - accuracy: 0.7702\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 35/100 [=========>....................] - ETA: 21:30 - loss: 0.5496 - accuracy: 0.7661\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 36/100 [=========>....................] - ETA: 21:11 - loss: 0.5479 - accuracy: 0.7674\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 37/100 [==========>...................] - ETA: 20:51 - loss: 0.5463 - accuracy: 0.7686\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 38/100 [==========>...................] - ETA: 20:31 - loss: 0.5468 - accuracy: 0.7681\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 39/100 [==========>...................] - ETA: 20:12 - loss: 0.5472 - accuracy: 0.7676\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 40/100 [===========>..................] - ETA: 19:52 - loss: 0.5476 - accuracy: 0.7672\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 41/100 [===========>..................] - ETA: 19:32 - loss: 0.5479 - accuracy: 0.7668\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 42/100 [===========>..................] - ETA: 19:12 - loss: 0.5483 - accuracy: 0.7664\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 43/100 [===========>..................] - ETA: 18:52 - loss: 0.5486 - accuracy: 0.7660\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 44/100 [============>.................] - ETA: 18:32 - loss: 0.5489 - accuracy: 0.7656\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 45/100 [============>.................] - ETA: 18:13 - loss: 0.5477 - accuracy: 0.7667\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 46/100 [============>.................] - ETA: 17:53 - loss: 0.5510 - accuracy: 0.7636\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 47/100 [=============>................] - ETA: 17:32 - loss: 0.5557 - accuracy: 0.7593\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 48/100 [=============>................] - ETA: 17:11 - loss: 0.5515 - accuracy: 0.7630\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 49/100 [=============>................] - ETA: 16:51 - loss: 0.5518 - accuracy: 0.7628\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 50/100 [==============>...............] - ETA: 16:32 - loss: 0.5547 - accuracy: 0.7600\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 51/100 [==============>...............] - ETA: 16:19 - loss: 0.5562 - accuracy: 0.7586\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 52/100 [==============>...............] - ETA: 16:05 - loss: 0.5576 - accuracy: 0.7572\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 53/100 [==============>...............] - ETA: 15:48 - loss: 0.5589 - accuracy: 0.7559\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 54/100 [===============>..............] - ETA: 15:34 - loss: 0.5590 - accuracy: 0.7558\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 55/100 [===============>..............] - ETA: 15:18 - loss: 0.5579 - accuracy: 0.7568\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 56/100 [===============>..............] - ETA: 15:03 - loss: 0.5557 - accuracy: 0.7589\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 57/100 [================>.............] - ETA: 14:48 - loss: 0.5547 - accuracy: 0.7599\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 58/100 [================>.............] - ETA: 14:30 - loss: 0.5548 - accuracy: 0.7597\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 59/100 [================>.............] - ETA: 14:14 - loss: 0.5560 - accuracy: 0.7585\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 60/100 [=================>............] - ETA: 13:57 - loss: 0.5540 - accuracy: 0.7604\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 61/100 [=================>............] - ETA: 13:41 - loss: 0.5552 - accuracy: 0.7592\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 62/100 [=================>............] - ETA: 13:22 - loss: 0.5564 - accuracy: 0.7581\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 63/100 [=================>............] - ETA: 13:02 - loss: 0.5565 - accuracy: 0.7579\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 64/100 [==================>...........] - ETA: 12:43 - loss: 0.5566 - accuracy: 0.7578\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 65/100 [==================>...........] - ETA: 12:24 - loss: 0.5556 - accuracy: 0.7587\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 66/100 [==================>...........] - ETA: 12:04 - loss: 0.5528 - accuracy: 0.7614\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 67/100 [===================>..........] - ETA: 11:43 - loss: 0.5539 - accuracy: 0.7603\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 68/100 [===================>..........] - ETA: 11:21 - loss: 0.5560 - accuracy: 0.7583\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 69/100 [===================>..........] - ETA: 11:01 - loss: 0.5561 - accuracy: 0.7582\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 70/100 [====================>.........] - ETA: 10:40 - loss: 0.5562 - accuracy: 0.7580\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 71/100 [====================>.........] - ETA: 10:18 - loss: 0.5572 - accuracy: 0.7570\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 72/100 [====================>.........] - ETA: 9:57 - loss: 0.5582 - accuracy: 0.7561 \n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 73/100 [====================>.........] - ETA: 9:35 - loss: 0.5564 - accuracy: 0.7577\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 74/100 [=====================>........] - ETA: 9:13 - loss: 0.5592 - accuracy: 0.7551\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 75/100 [=====================>........] - ETA: 8:52 - loss: 0.5610 - accuracy: 0.7533\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 76/100 [=====================>........] - ETA: 8:31 - loss: 0.5645 - accuracy: 0.7500\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 77/100 [======================>.......] - ETA: 8:09 - loss: 0.5637 - accuracy: 0.7508\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 78/100 [======================>.......] - ETA: 7:47 - loss: 0.5636 - accuracy: 0.7508\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 79/100 [======================>.......] - ETA: 7:27 - loss: 0.5661 - accuracy: 0.7484\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 80/100 [=======================>......] - ETA: 7:05 - loss: 0.5692 - accuracy: 0.7453\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 81/100 [=======================>......] - ETA: 6:44 - loss: 0.5691 - accuracy: 0.7454\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 82/100 [=======================>......] - ETA: 6:23 - loss: 0.5690 - accuracy: 0.7454\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 83/100 [=======================>......] - ETA: 6:03 - loss: 0.5690 - accuracy: 0.7455\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 84/100 [========================>.....] - ETA: 5:42 - loss: 0.5675 - accuracy: 0.7470\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 85/100 [========================>.....] - ETA: 5:22 - loss: 0.5689 - accuracy: 0.7456\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 86/100 [========================>.....] - ETA: 5:02 - loss: 0.5695 - accuracy: 0.7449\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 87/100 [=========================>....] - ETA: 4:42 - loss: 0.5688 - accuracy: 0.7457\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 88/100 [=========================>....] - ETA: 4:21 - loss: 0.5701 - accuracy: 0.7443\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 89/100 [=========================>....] - ETA: 4:00 - loss: 0.5713 - accuracy: 0.7430\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 90/100 [==========================>...] - ETA: 3:39 - loss: 0.5706 - accuracy: 0.7437\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 91/100 [==========================>...] - ETA: 3:18 - loss: 0.5705 - accuracy: 0.7438\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 92/100 [==========================>...] - ETA: 2:57 - loss: 0.5717 - accuracy: 0.7425\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 93/100 [==========================>...] - ETA: 2:35 - loss: 0.5723 - accuracy: 0.7419\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 94/100 [===========================>..] - ETA: 2:13 - loss: 0.5716 - accuracy: 0.7427\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 95/100 [===========================>..] - ETA: 1:52 - loss: 0.5721 - accuracy: 0.7421\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 96/100 [===========================>..] - ETA: 1:29 - loss: 0.5727 - accuracy: 0.7415\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 97/100 [============================>.] - ETA: 1:07 - loss: 0.5732 - accuracy: 0.7410\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 98/100 [============================>.] - ETA: 44s - loss: 0.5714 - accuracy: 0.7430 \n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      " 99/100 [============================>.] - ETA: 22s - loss: 0.5719 - accuracy: 0.7424\n",
      "Epoch 00013: accuracy did not improve from 0.96875\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "100/100 [==============================] - 2294s 23s/step - loss: 0.5719 - accuracy: 0.7425 - val_loss: 0.6617 - val_accuracy: 0.6438\n",
      "Epoch 14/25\n",
      "\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      "  1/100 [..............................] - ETA: 29:56 - loss: 0.5662 - accuracy: 0.7500\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      "  2/100 [..............................] - ETA: 30:17 - loss: 0.5098 - accuracy: 0.8125\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      "  3/100 [..............................] - ETA: 30:35 - loss: 0.4908 - accuracy: 0.8333\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      "  4/100 [>.............................] - ETA: 30:43 - loss: 0.5238 - accuracy: 0.7969\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      "  5/100 [>.............................] - ETA: 30:31 - loss: 0.5436 - accuracy: 0.7750\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      "  6/100 [>.............................] - ETA: 30:09 - loss: 0.5472 - accuracy: 0.7708\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      "  7/100 [=>............................] - ETA: 29:31 - loss: 0.5747 - accuracy: 0.7411\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      "  8/100 [=>............................] - ETA: 29:16 - loss: 0.5589 - accuracy: 0.7578\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      "  9/100 [=>............................] - ETA: 29:01 - loss: 0.5596 - accuracy: 0.7569\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 10/100 [==>...........................] - ETA: 28:38 - loss: 0.5483 - accuracy: 0.7688\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 11/100 [==>...........................] - ETA: 28:17 - loss: 0.5606 - accuracy: 0.7557\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 12/100 [==>...........................] - ETA: 27:55 - loss: 0.5858 - accuracy: 0.7292\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 13/100 [==>...........................] - ETA: 27:35 - loss: 0.5749 - accuracy: 0.7404\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 14/100 [===>..........................] - ETA: 27:07 - loss: 0.5742 - accuracy: 0.7411\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 15/100 [===>..........................] - ETA: 26:47 - loss: 0.5655 - accuracy: 0.7500\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 16/100 [===>..........................] - ETA: 26:29 - loss: 0.5730 - accuracy: 0.7422\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 17/100 [====>.........................] - ETA: 26:06 - loss: 0.5653 - accuracy: 0.7500\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 18/100 [====>.........................] - ETA: 25:39 - loss: 0.5720 - accuracy: 0.7431\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 19/100 [====>.........................] - ETA: 25:19 - loss: 0.5780 - accuracy: 0.7368\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 20/100 [=====>........................] - ETA: 25:02 - loss: 0.5895 - accuracy: 0.7250\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 21/100 [=====>........................] - ETA: 24:46 - loss: 0.5911 - accuracy: 0.7232\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 22/100 [=====>........................] - ETA: 24:24 - loss: 0.5899 - accuracy: 0.7244\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 23/100 [=====>........................] - ETA: 24:08 - loss: 0.5862 - accuracy: 0.7283\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 24/100 [======>.......................] - ETA: 23:52 - loss: 0.5828 - accuracy: 0.7318\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 25/100 [======>.......................] - ETA: 23:33 - loss: 0.5844 - accuracy: 0.7300\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26/100 [======>.......................] - ETA: 23:13 - loss: 0.5836 - accuracy: 0.7308\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 27/100 [=======>......................] - ETA: 22:54 - loss: 0.5763 - accuracy: 0.7384\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 28/100 [=======>......................] - ETA: 22:35 - loss: 0.5801 - accuracy: 0.7344\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 29/100 [=======>......................] - ETA: 22:14 - loss: 0.5754 - accuracy: 0.7392\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 30/100 [========>.....................] - ETA: 21:54 - loss: 0.5750 - accuracy: 0.7396\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 31/100 [========>.....................] - ETA: 21:34 - loss: 0.5746 - accuracy: 0.7399\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 32/100 [========>.....................] - ETA: 21:14 - loss: 0.5762 - accuracy: 0.7383\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 33/100 [========>.....................] - ETA: 20:54 - loss: 0.5740 - accuracy: 0.7405\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 34/100 [=========>....................] - ETA: 20:34 - loss: 0.5718 - accuracy: 0.7426\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 35/100 [=========>....................] - ETA: 20:13 - loss: 0.5680 - accuracy: 0.7464\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 36/100 [=========>....................] - ETA: 19:53 - loss: 0.5679 - accuracy: 0.7465\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 37/100 [==========>...................] - ETA: 19:36 - loss: 0.5677 - accuracy: 0.7466\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 38/100 [==========>...................] - ETA: 19:15 - loss: 0.5710 - accuracy: 0.7434\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 39/100 [==========>...................] - ETA: 18:55 - loss: 0.5774 - accuracy: 0.7372\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 40/100 [===========>..................] - ETA: 18:36 - loss: 0.5754 - accuracy: 0.7391\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 41/100 [===========>..................] - ETA: 18:20 - loss: 0.5736 - accuracy: 0.7409\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 42/100 [===========>..................] - ETA: 18:01 - loss: 0.5702 - accuracy: 0.7440\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 43/100 [===========>..................] - ETA: 17:50 - loss: 0.5670 - accuracy: 0.7471\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 44/100 [============>.................] - ETA: 17:31 - loss: 0.5669 - accuracy: 0.7472\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 45/100 [============>.................] - ETA: 17:12 - loss: 0.5609 - accuracy: 0.7528\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 46/100 [============>.................] - ETA: 16:54 - loss: 0.5639 - accuracy: 0.7500\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 47/100 [=============>................] - ETA: 16:37 - loss: 0.5638 - accuracy: 0.7500\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 48/100 [=============>................] - ETA: 16:27 - loss: 0.5638 - accuracy: 0.7500\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 49/100 [=============>................] - ETA: 16:07 - loss: 0.5624 - accuracy: 0.7513\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 50/100 [==============>...............] - ETA: 15:48 - loss: 0.5596 - accuracy: 0.7538\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 51/100 [==============>...............] - ETA: 15:30 - loss: 0.5638 - accuracy: 0.7500\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 52/100 [==============>...............] - ETA: 15:11 - loss: 0.5679 - accuracy: 0.7464\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 53/100 [==============>...............] - ETA: 14:55 - loss: 0.5651 - accuracy: 0.7488\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 54/100 [===============>..............] - ETA: 14:37 - loss: 0.5650 - accuracy: 0.7488\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 55/100 [===============>..............] - ETA: 14:17 - loss: 0.5637 - accuracy: 0.7500\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 56/100 [===============>..............] - ETA: 13:58 - loss: 0.5650 - accuracy: 0.7489\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 57/100 [================>.............] - ETA: 13:39 - loss: 0.5624 - accuracy: 0.7511\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 58/100 [================>.............] - ETA: 13:20 - loss: 0.5636 - accuracy: 0.7500\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 59/100 [================>.............] - ETA: 13:02 - loss: 0.5624 - accuracy: 0.7511\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 60/100 [=================>............] - ETA: 12:44 - loss: 0.5636 - accuracy: 0.7500\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 61/100 [=================>............] - ETA: 12:26 - loss: 0.5612 - accuracy: 0.7520\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 62/100 [=================>............] - ETA: 12:07 - loss: 0.5589 - accuracy: 0.7540\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 63/100 [=================>............] - ETA: 11:48 - loss: 0.5578 - accuracy: 0.7550\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 64/100 [==================>...........] - ETA: 11:29 - loss: 0.5602 - accuracy: 0.7529\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 65/100 [==================>...........] - ETA: 11:10 - loss: 0.5614 - accuracy: 0.7519\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 66/100 [==================>...........] - ETA: 10:50 - loss: 0.5614 - accuracy: 0.7519\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 67/100 [===================>..........] - ETA: 10:31 - loss: 0.5592 - accuracy: 0.7537\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 68/100 [===================>..........] - ETA: 10:11 - loss: 0.5560 - accuracy: 0.7564\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 69/100 [===================>..........] - ETA: 9:52 - loss: 0.5583 - accuracy: 0.7545 \n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 70/100 [====================>.........] - ETA: 9:33 - loss: 0.5562 - accuracy: 0.7563\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 71/100 [====================>.........] - ETA: 9:14 - loss: 0.5584 - accuracy: 0.7544\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 72/100 [====================>.........] - ETA: 8:56 - loss: 0.5585 - accuracy: 0.7543\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 73/100 [====================>.........] - ETA: 8:37 - loss: 0.5555 - accuracy: 0.7568\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 74/100 [=====================>........] - ETA: 8:19 - loss: 0.5546 - accuracy: 0.7576\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 75/100 [=====================>........] - ETA: 8:00 - loss: 0.5526 - accuracy: 0.7592\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 76/100 [=====================>........] - ETA: 7:41 - loss: 0.5518 - accuracy: 0.7599\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 77/100 [======================>.......] - ETA: 7:22 - loss: 0.5499 - accuracy: 0.7614\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 78/100 [======================>.......] - ETA: 7:03 - loss: 0.5511 - accuracy: 0.7604\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 79/100 [======================>.......] - ETA: 6:44 - loss: 0.5503 - accuracy: 0.7611\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 80/100 [=======================>......] - ETA: 6:26 - loss: 0.5515 - accuracy: 0.7602\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 81/100 [=======================>......] - ETA: 6:07 - loss: 0.5517 - accuracy: 0.7600\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 82/100 [=======================>......] - ETA: 5:47 - loss: 0.5549 - accuracy: 0.7576\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 83/100 [=======================>......] - ETA: 5:28 - loss: 0.5530 - accuracy: 0.7590\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 84/100 [========================>.....] - ETA: 5:09 - loss: 0.5541 - accuracy: 0.7582\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 85/100 [========================>.....] - ETA: 4:50 - loss: 0.5533 - accuracy: 0.7588\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 86/100 [========================>.....] - ETA: 4:30 - loss: 0.5544 - accuracy: 0.7580\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 87/100 [=========================>....] - ETA: 4:11 - loss: 0.5563 - accuracy: 0.7565\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 88/100 [=========================>....] - ETA: 3:52 - loss: 0.5564 - accuracy: 0.7564\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 89/100 [=========================>....] - ETA: 3:33 - loss: 0.5573 - accuracy: 0.7556\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 90/100 [==========================>...] - ETA: 3:14 - loss: 0.5574 - accuracy: 0.7556\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 91/100 [==========================>...] - ETA: 2:55 - loss: 0.5567 - accuracy: 0.7562\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 92/100 [==========================>...] - ETA: 2:35 - loss: 0.5559 - accuracy: 0.7568\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 93/100 [==========================>...] - ETA: 2:16 - loss: 0.5537 - accuracy: 0.7587\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 94/100 [===========================>..] - ETA: 1:56 - loss: 0.5545 - accuracy: 0.7580\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 95/100 [===========================>..] - ETA: 1:37 - loss: 0.5546 - accuracy: 0.7579\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 96/100 [===========================>..] - ETA: 1:18 - loss: 0.5540 - accuracy: 0.7585\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 97/100 [============================>.] - ETA: 58s - loss: 0.5555 - accuracy: 0.7571 \n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 98/100 [============================>.] - ETA: 39s - loss: 0.5556 - accuracy: 0.7570\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      " 99/100 [============================>.] - ETA: 19s - loss: 0.5550 - accuracy: 0.7576\n",
      "Epoch 00014: accuracy did not improve from 0.96875\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "100/100 [==============================] - 2023s 20s/step - loss: 0.5571 - accuracy: 0.7556 - val_loss: 0.6803 - val_accuracy: 0.6438\n",
      "Epoch 15/25\n",
      "\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      "  1/100 [..............................] - ETA: 39:57 - loss: 0.5623 - accuracy: 0.7500\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      "  2/100 [..............................] - ETA: 35:39 - loss: 0.5279 - accuracy: 0.7812\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      "  3/100 [..............................] - ETA: 34:02 - loss: 0.5622 - accuracy: 0.7500\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      "  4/100 [>.............................] - ETA: 36:40 - loss: 0.5453 - accuracy: 0.7656\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      "  5/100 [>.............................] - ETA: 37:46 - loss: 0.5487 - accuracy: 0.7625\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      "  6/100 [>.............................] - ETA: 38:41 - loss: 0.5622 - accuracy: 0.7500\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      "  7/100 [=>............................] - ETA: 38:48 - loss: 0.5431 - accuracy: 0.7679\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      "  8/100 [=>............................] - ETA: 39:01 - loss: 0.5622 - accuracy: 0.7500\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      "  9/100 [=>............................] - ETA: 39:03 - loss: 0.5697 - accuracy: 0.7431\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 10/100 [==>...........................] - ETA: 38:58 - loss: 0.5756 - accuracy: 0.7375\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 11/100 [==>...........................] - ETA: 39:09 - loss: 0.5744 - accuracy: 0.7386\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 12/100 [==>...........................] - ETA: 39:03 - loss: 0.5789 - accuracy: 0.7344\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 13/100 [==>...........................] - ETA: 38:53 - loss: 0.5877 - accuracy: 0.7260\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 14/100 [===>..........................] - ETA: 38:38 - loss: 0.5998 - accuracy: 0.7143\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 15/100 [===>..........................] - ETA: 38:23 - loss: 0.6143 - accuracy: 0.7000\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 16/100 [===>..........................] - ETA: 38:07 - loss: 0.6072 - accuracy: 0.7070\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 17/100 [====>.........................] - ETA: 37:42 - loss: 0.6119 - accuracy: 0.7022\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 18/100 [====>.........................] - ETA: 37:21 - loss: 0.6092 - accuracy: 0.7049\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 19/100 [====>.........................] - ETA: 36:57 - loss: 0.6005 - accuracy: 0.7138\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 20/100 [=====>........................] - ETA: 36:37 - loss: 0.6077 - accuracy: 0.7063\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 21/100 [=====>........................] - ETA: 36:16 - loss: 0.6113 - accuracy: 0.7024\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 22/100 [=====>........................] - ETA: 35:54 - loss: 0.6065 - accuracy: 0.7074\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 23/100 [=====>........................] - ETA: 35:29 - loss: 0.6072 - accuracy: 0.7065\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 24/100 [======>.......................] - ETA: 35:02 - loss: 0.6103 - accuracy: 0.7031\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 25/100 [======>.......................] - ETA: 34:28 - loss: 0.6085 - accuracy: 0.7050\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 26/100 [======>.......................] - ETA: 33:59 - loss: 0.6069 - accuracy: 0.7067\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 27/100 [=======>......................] - ETA: 33:35 - loss: 0.6116 - accuracy: 0.7014\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 28/100 [=======>......................] - ETA: 33:09 - loss: 0.6140 - accuracy: 0.6987\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 29/100 [=======>......................] - ETA: 32:45 - loss: 0.6162 - accuracy: 0.6961\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 30/100 [========>.....................] - ETA: 32:20 - loss: 0.6164 - accuracy: 0.6958\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 31/100 [========>.....................] - ETA: 31:55 - loss: 0.6183 - accuracy: 0.6935\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 32/100 [========>.....................] - ETA: 31:32 - loss: 0.6167 - accuracy: 0.6953\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 33/100 [========>.....................] - ETA: 31:10 - loss: 0.6104 - accuracy: 0.7027\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 34/100 [=========>....................] - ETA: 30:47 - loss: 0.6045 - accuracy: 0.7096\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 35/100 [=========>....................] - ETA: 30:21 - loss: 0.6050 - accuracy: 0.7089\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 36/100 [=========>....................] - ETA: 30:00 - loss: 0.6054 - accuracy: 0.7083\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 37/100 [==========>...................] - ETA: 29:34 - loss: 0.6073 - accuracy: 0.7061\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 38/100 [==========>...................] - ETA: 29:09 - loss: 0.6035 - accuracy: 0.7105\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 39/100 [==========>...................] - ETA: 28:42 - loss: 0.5970 - accuracy: 0.7179\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 40/100 [===========>..................] - ETA: 28:16 - loss: 0.5936 - accuracy: 0.7219\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 41/100 [===========>..................] - ETA: 27:50 - loss: 0.5943 - accuracy: 0.7210\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42/100 [===========>..................] - ETA: 27:23 - loss: 0.5949 - accuracy: 0.7202\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 43/100 [===========>..................] - ETA: 26:56 - loss: 0.5956 - accuracy: 0.7195\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 44/100 [============>.................] - ETA: 26:29 - loss: 0.5962 - accuracy: 0.7188\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 45/100 [============>.................] - ETA: 26:01 - loss: 0.5942 - accuracy: 0.7208\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 46/100 [============>.................] - ETA: 25:34 - loss: 0.5924 - accuracy: 0.7228\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 47/100 [=============>................] - ETA: 25:09 - loss: 0.5930 - accuracy: 0.7221\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 48/100 [=============>................] - ETA: 24:44 - loss: 0.5949 - accuracy: 0.7201\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 49/100 [=============>................] - ETA: 24:19 - loss: 0.5943 - accuracy: 0.7207\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 50/100 [==============>...............] - ETA: 23:52 - loss: 0.5960 - accuracy: 0.7188\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 51/100 [==============>...............] - ETA: 23:26 - loss: 0.5954 - accuracy: 0.7194\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 52/100 [==============>...............] - ETA: 23:00 - loss: 0.5925 - accuracy: 0.7224\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 53/100 [==============>...............] - ETA: 22:33 - loss: 0.5909 - accuracy: 0.7241\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 54/100 [===============>..............] - ETA: 22:08 - loss: 0.5904 - accuracy: 0.7245\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 55/100 [===============>..............] - ETA: 21:39 - loss: 0.5878 - accuracy: 0.7273\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 56/100 [===============>..............] - ETA: 23:32 - loss: 0.5906 - accuracy: 0.7243\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 57/100 [================>.............] - ETA: 22:51 - loss: 0.5880 - accuracy: 0.7270\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 58/100 [================>.............] - ETA: 22:09 - loss: 0.5886 - accuracy: 0.7263\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 59/100 [================>.............] - ETA: 21:29 - loss: 0.5882 - accuracy: 0.7267\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 60/100 [=================>............] - ETA: 20:47 - loss: 0.5909 - accuracy: 0.7240\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 61/100 [=================>............] - ETA: 20:08 - loss: 0.5904 - accuracy: 0.7244\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 62/100 [=================>............] - ETA: 19:31 - loss: 0.5900 - accuracy: 0.7248\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 63/100 [=================>............] - ETA: 18:54 - loss: 0.5925 - accuracy: 0.7222\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 64/100 [==================>...........] - ETA: 18:17 - loss: 0.5931 - accuracy: 0.7217\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 65/100 [==================>...........] - ETA: 17:41 - loss: 0.5936 - accuracy: 0.7212\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 66/100 [==================>...........] - ETA: 17:05 - loss: 0.5922 - accuracy: 0.7225\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 67/100 [===================>..........] - ETA: 16:29 - loss: 0.5936 - accuracy: 0.7211\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 68/100 [===================>..........] - ETA: 15:55 - loss: 0.5931 - accuracy: 0.7215\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 69/100 [===================>..........] - ETA: 15:20 - loss: 0.5909 - accuracy: 0.7237\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 70/100 [====================>.........] - ETA: 14:47 - loss: 0.5905 - accuracy: 0.7241\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 71/100 [====================>.........] - ETA: 14:14 - loss: 0.5910 - accuracy: 0.7236\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 72/100 [====================>.........] - ETA: 13:42 - loss: 0.5889 - accuracy: 0.7257\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 73/100 [====================>.........] - ETA: 13:09 - loss: 0.5877 - accuracy: 0.7269\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 74/100 [=====================>........] - ETA: 12:37 - loss: 0.5874 - accuracy: 0.7272\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 75/100 [=====================>........] - ETA: 12:06 - loss: 0.5888 - accuracy: 0.7258\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 76/100 [=====================>........] - ETA: 11:34 - loss: 0.5876 - accuracy: 0.7270\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 77/100 [======================>.......] - ETA: 11:03 - loss: 0.5889 - accuracy: 0.7256\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 78/100 [======================>.......] - ETA: 10:32 - loss: 0.5902 - accuracy: 0.7244\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 79/100 [======================>.......] - ETA: 10:01 - loss: 0.5938 - accuracy: 0.7207\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 80/100 [=======================>......] - ETA: 9:30 - loss: 0.5934 - accuracy: 0.7211 \n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 81/100 [=======================>......] - ETA: 9:00 - loss: 0.5938 - accuracy: 0.7207\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 82/100 [=======================>......] - ETA: 8:30 - loss: 0.5935 - accuracy: 0.7210\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 83/100 [=======================>......] - ETA: 8:00 - loss: 0.5946 - accuracy: 0.7199\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 84/100 [========================>.....] - ETA: 7:30 - loss: 0.5942 - accuracy: 0.7202\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 85/100 [========================>.....] - ETA: 7:01 - loss: 0.5953 - accuracy: 0.7191\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 86/100 [========================>.....] - ETA: 6:32 - loss: 0.5928 - accuracy: 0.7217\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 87/100 [=========================>....] - ETA: 6:03 - loss: 0.5939 - accuracy: 0.7205\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 88/100 [=========================>....] - ETA: 5:34 - loss: 0.5929 - accuracy: 0.7216\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 89/100 [=========================>....] - ETA: 5:05 - loss: 0.5932 - accuracy: 0.7212\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 90/100 [==========================>...] - ETA: 4:37 - loss: 0.5929 - accuracy: 0.7215\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 91/100 [==========================>...] - ETA: 4:08 - loss: 0.5919 - accuracy: 0.7225\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 92/100 [==========================>...] - ETA: 3:40 - loss: 0.5904 - accuracy: 0.7242\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 93/100 [==========================>...] - ETA: 3:12 - loss: 0.5895 - accuracy: 0.7251\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 94/100 [===========================>..] - ETA: 2:44 - loss: 0.5892 - accuracy: 0.7254\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 95/100 [===========================>..] - ETA: 2:16 - loss: 0.5908 - accuracy: 0.7237\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 96/100 [===========================>..] - ETA: 1:49 - loss: 0.5905 - accuracy: 0.7240\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 97/100 [============================>.] - ETA: 1:21 - loss: 0.5884 - accuracy: 0.7262\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 98/100 [============================>.] - ETA: 54s - loss: 0.5888 - accuracy: 0.7258 \n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      " 99/100 [============================>.] - ETA: 27s - loss: 0.5898 - accuracy: 0.7247\n",
      "Epoch 00015: accuracy did not improve from 0.96875\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "100/100 [==============================] - 2758s 28s/step - loss: 0.5901 - accuracy: 0.7244 - val_loss: 0.6671 - val_accuracy: 0.6438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25\n",
      "\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      "  1/100 [..............................] - ETA: 35:47 - loss: 0.3817 - accuracy: 0.9375\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      "  2/100 [..............................] - ETA: 36:44 - loss: 0.4422 - accuracy: 0.8750\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      "  3/100 [..............................] - ETA: 40:33 - loss: 0.5442 - accuracy: 0.7708\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      "  4/100 [>.............................] - ETA: 42:05 - loss: 0.6261 - accuracy: 0.6875\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      "  5/100 [>.............................] - ETA: 42:52 - loss: 0.6013 - accuracy: 0.7125\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      "  6/100 [>.............................] - ETA: 43:28 - loss: 0.6155 - accuracy: 0.6979\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      "  7/100 [=>............................] - ETA: 43:25 - loss: 0.5907 - accuracy: 0.7232\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      "  8/100 [=>............................] - ETA: 43:06 - loss: 0.6025 - accuracy: 0.7109\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      "  9/100 [=>............................] - ETA: 42:46 - loss: 0.5982 - accuracy: 0.7153\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      " 10/100 [==>...........................] - ETA: 42:28 - loss: 0.6130 - accuracy: 0.7000\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      " 11/100 [==>...........................] - ETA: 42:14 - loss: 0.6086 - accuracy: 0.7045\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      " 12/100 [==>...........................] - ETA: 42:00 - loss: 0.5848 - accuracy: 0.7292\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      " 13/100 [==>...........................] - ETA: 41:45 - loss: 0.5878 - accuracy: 0.7260\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      " 14/100 [===>..........................] - ETA: 41:26 - loss: 0.5818 - accuracy: 0.7321\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      " 15/100 [===>..........................] - ETA: 41:05 - loss: 0.5766 - accuracy: 0.7375\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      " 16/100 [===>..........................] - ETA: 40:41 - loss: 0.5720 - accuracy: 0.7422\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      " 17/100 [====>.........................] - ETA: 40:17 - loss: 0.5715 - accuracy: 0.7426\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      " 18/100 [====>.........................] - ETA: 40:02 - loss: 0.5676 - accuracy: 0.7465\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      " 19/100 [====>.........................] - ETA: 39:50 - loss: 0.5609 - accuracy: 0.7533\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      " 20/100 [=====>........................] - ETA: 39:35 - loss: 0.5641 - accuracy: 0.7500\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      " 21/100 [=====>........................] - ETA: 39:16 - loss: 0.5611 - accuracy: 0.7530\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      " 22/100 [=====>........................] - ETA: 38:51 - loss: 0.5611 - accuracy: 0.7528\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      " 23/100 [=====>........................] - ETA: 38:33 - loss: 0.5612 - accuracy: 0.7527\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      " 24/100 [======>.......................] - ETA: 38:10 - loss: 0.5559 - accuracy: 0.7578\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      " 25/100 [======>.......................] - ETA: 37:45 - loss: 0.5561 - accuracy: 0.7575\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      " 26/100 [======>.......................] - ETA: 37:20 - loss: 0.5615 - accuracy: 0.7524\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      " 27/100 [=======>......................] - ETA: 36:52 - loss: 0.5566 - accuracy: 0.7569\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      " 28/100 [=======>......................] - ETA: 36:29 - loss: 0.5616 - accuracy: 0.7522\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      " 29/100 [=======>......................] - ETA: 36:05 - loss: 0.5616 - accuracy: 0.7522\n",
      "Epoch 00016: accuracy did not improve from 0.96875\n",
      " 30/100 [========>.....................] - ETA: 35:37 - loss: 0.5730 - accuracy: 0.7417"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(steps_per_epoch=100,\n",
    "                              generator=train_set, \n",
    "                              validation_data=test_set, \n",
    "                              validation_steps=10,\n",
    "                              epochs=25,\n",
    "                              callbacks=[checkpoint,early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(25)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
